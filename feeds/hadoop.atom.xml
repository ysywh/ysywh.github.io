<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>YsY记录点滴</title><link href="http://ysywh.github.io/" rel="alternate"></link><link href="http://ysywh.github.io/feeds/hadoop.atom.xml" rel="self"></link><id>http://ysywh.github.io/</id><updated>2015-10-01T17:30:00+08:00</updated><entry><title>Mapreduce API 操作HBase</title><link href="http://ysywh.github.io/Mapreduce%20API%20%E6%93%8D%E4%BD%9CHBase.html" rel="alternate"></link><updated>2015-08-30T17:30:00+08:00</updated><author><name>ysy</name></author><id>tag:ysywh.github.io,2015-06-03:Mapreduce API 操作HBase.html</id><summary type="html">&lt;h3&gt;&lt;strong&gt;前言&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;之前博文采用shell直接操作Hbase，这次通过一些API函数对habse进行操作。主要完成：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;用JAVA API完成对hbase表的建立、删除、数据添加与查看等基本操作&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;用MapReduce完成hdfs数据向habse表的导入&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;通过写java代码，打成jar包，然后直接运行即可。&lt;/p&gt;
&lt;h3&gt;&lt;strong&gt;一 用JAVA API完成对hbase表的建立、删除、数据添加与查看等基本操作&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;这里只对简单的几个操作函数（建表，添加数据）进行分析，其他类似：&lt;/p&gt;
&lt;p&gt;其中用到了几个和Hbase相关的类（HBaseAdmin、HBaseConfiguration、HTable、HTableDescriptor、Put、Get、Scanner等）与其成员函数，具体含义可以看这个&lt;a href="http://blog.csdn.net/lifuxiangcaohui/article/details/39997205"&gt;链接&lt;/a&gt;，讲解很明白。&lt;/p&gt;
&lt;p&gt;1 建立管理habse的接口对象&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;public class hbase{
static Configuration conf = HBaseConfiguration.create();
static HBaseAdmin admin;
public static void main(String[] args){
    try {
        admin = new HBaseAdmin(conf);
    } catch (IOException e) {
        e.printStackTrace();
    }

    createTable(&amp;quot;access_yansiyu2&amp;quot;);
    addRow(&amp;quot;access_yansiyu2&amp;quot;);
    ...
    }
...
}
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;分析：先得到一个缺省的“配置”对象conf，然后用类HBaseAdmin构造对象admin,HBaseAdmin提供了管理HBase数据库的表信息的接口，因此之后通过操作admin就可以对表进行部分操作，比如建立、删除表等。但是，对于向表中添加删除数据等操作用HTable类对象进行操作，后面会遇到。&lt;/p&gt;
&lt;p&gt;类HBaseAdmin有很多表操作的api成员函数，比如createTable创建表，deleteTable删除表，添加列addColumn,enableTable使表有效，使表无效disableTable等等。&lt;/p&gt;
&lt;p&gt;2 创建表&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="nt"&gt;public&lt;/span&gt; &lt;span class="nt"&gt;static&lt;/span&gt; &lt;span class="nt"&gt;void&lt;/span&gt; &lt;span class="nt"&gt;createTable&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="nt"&gt;String&lt;/span&gt; &lt;span class="nt"&gt;tablename&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;
    &lt;span class="n"&gt;try&lt;/span&gt; &lt;span class="err"&gt;{&lt;/span&gt;
        &lt;span class="n"&gt;if&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;admin&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;tableExists&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;tablename&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;&lt;span class="err"&gt;{&lt;/span&gt;  
            &lt;span class="n"&gt;System&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;out&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;println&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;table is existed&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
        &lt;span class="p"&gt;}&lt;/span&gt;&lt;span class="nt"&gt;else&lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;  
            &lt;span class="n"&gt;HTableDescriptor&lt;/span&gt; &lt;span class="n"&gt;desc&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;new&lt;/span&gt; &lt;span class="n"&gt;HTableDescriptor&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;Bytes&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;toBytes&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;tablename&lt;/span&gt;&lt;span class="p"&gt;));&lt;/span&gt;
            &lt;span class="n"&gt;HColumnDescriptor&lt;/span&gt; &lt;span class="n"&gt;coldef&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;new&lt;/span&gt; &lt;span class="n"&gt;HColumnDescriptor&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;Bytes&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;toBytes&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;colfamily&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;));&lt;/span&gt;
            &lt;span class="n"&gt;desc&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;addFamily&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;coldef&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
            &lt;span class="n"&gt;admin&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;createTable&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;desc&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
            &lt;span class="n"&gt;Boolean&lt;/span&gt; &lt;span class="n"&gt;avail&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;admin&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;isTableAvailable&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;Bytes&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;toBytes&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;tablename&lt;/span&gt;&lt;span class="p"&gt;));&lt;/span&gt;
            &lt;span class="n"&gt;System&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;out&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;println&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;Table create: &amp;quot;&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;avail&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
        &lt;span class="p"&gt;}&lt;/span&gt;
    &lt;span class="err"&gt;}&lt;/span&gt; &lt;span class="nt"&gt;catch&lt;/span&gt; &lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="nt"&gt;IOException&lt;/span&gt; &lt;span class="nt"&gt;e&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
        &lt;span class="o"&gt;//&lt;/span&gt; &lt;span class="n"&gt;TODO&lt;/span&gt; &lt;span class="n"&gt;Auto&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;generated&lt;/span&gt; &lt;span class="n"&gt;catch&lt;/span&gt; &lt;span class="k"&gt;block&lt;/span&gt;
        &lt;span class="n"&gt;e&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;printStackTrace&lt;/span&gt;&lt;span class="p"&gt;();&lt;/span&gt;
    &lt;span class="p"&gt;}&lt;/span&gt;
&lt;span class="err"&gt;}&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;分析：通过第一步，这一步就可以创建表，需要调用admin的建表函数createTable即可。当然要实例化一个表描述器desc和列族描述器coldef,并且向desc中添加coldef,然后admin.createTable(desc)即可建立表。&lt;/p&gt;
&lt;p&gt;3 添加数据&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;public static void addRow(String tablename){
    try {
        HTable table = new HTable(conf,tablename);
        Put put = new Put(Bytes.toBytes(&amp;quot;row1&amp;quot;));
        put.add(Bytes.toBytes(&amp;quot;colfamily&amp;quot;), Bytes.toBytes(&amp;quot;col1&amp;quot;), Bytes.toBytes(&amp;quot;value1&amp;quot;));
        put.add(Bytes.toBytes(&amp;quot;colfamily&amp;quot;), Bytes.toBytes(&amp;quot;col2&amp;quot;), Bytes.toBytes(&amp;quot;value2&amp;quot;));
        table.put(put);
    } catch (RetriesExhaustedWithDetailsException | InterruptedIOException e) {
        // TODO Auto-generated catch block
        e.printStackTrace();
    } catch (IOException e) {
        // TODO Auto-generated catch block
        e.printStackTrace();
    }
}
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;分析：这里有一个重要类HTable，可以用来和HBase表直接通信，比如向表中添加、删除、查看等操作，主要调用类HTable的成员函数即可，如put（添加数据），delete（删除数据）、get（获取指定行数据），getTableName获取表名，isTableEnabled检查表是否有有效等等。&lt;/p&gt;
&lt;p&gt;这里用到了put(Put put)，则传入数据要实例化类Put。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;实验过程&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;先把代码打成jar包（如hbase_api.jar），然后直接在利用命令&lt;code&gt;hadoop jar hbase_api.jar&lt;/code&gt;执行即可:&lt;/p&gt;
&lt;p&gt;先执行jar包：&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;img alt="2" src="http://gitlab.local/course/course_hadoop/uploads/a19fac284f9b812d2a1f4d9e93998527/2.png" /&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;查看建立的表和数据&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;img alt="1" src="http://gitlab.local/course/course_hadoop/uploads/3693cb7303a57932ca1a6ccedf4f83a5/1.png" /&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;发现表access_ysy已经建立，且加入两行数据。验证成功。&lt;/p&gt;
&lt;p&gt;&lt;/br&gt;
二 &lt;strong&gt;用MapReduce完成hdfs数据向habse表的导入&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;一中是通过api直接建立hbase表，并对表进行数据操作。但如果数据已经存在，如何导入，则显得尤为重要。这里将通过mapreduce，将hadoop中已经存在的数据文件的数据导入hbase表中。&lt;/p&gt;
&lt;p&gt;主要通过map函数将数据读入，并输出到相同key值的reduce函数中，则遍历逐行的将每行的各个字段写入hbase表中即可。&lt;/p&gt;
&lt;p&gt;整个代码比较简单，主要有三个类，建立表的类、map类和reduce类。因为hbase表不存在，则先建立一个表：&lt;/p&gt;
&lt;p&gt;1 建表&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;private static final String targetTable = &amp;quot;mapreduce_yansiyu&amp;quot;;
static Configuration config = HBaseConfiguration.create();
//创建表，表名为 targetTable，列族列表为cfs
public static void createTable(String tablename, String[] cfs) throws IOException {
    //建表
    HBaseAdmin admin = new HBaseAdmin(config);
    if (admin.tableExists(tablename)) {
        System.out.println(&amp;quot;table already exists&amp;quot;);
    }
    else {
        HTableDescriptor tableDesc = new HTableDescriptor(tablename);
        for (int i = 0; i &amp;lt; cfs.length; i++) {
            //向表中加入列族
            tableDesc.addFamily(new HColumnDescriptor(cfs[i]));
        }
        //创建表
        admin.createTable(tableDesc);
        System.out.println(&amp;quot;create table successly&amp;quot;);
    }
}
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;2 map类&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt; public static class InputMapper extends Mapper&amp;lt;Object, Text, NullWritable, Text&amp;gt; {
    public void map(Object key, Text value, Context context) throws IOException, InterruptedException {
        //输出键值对的 键 为一样， 即空值。则map输出将进入同一个reduce中
        context.write(NullWritable.get(), value);
    }
}
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;可以看出读入的map中的数据，输出的键值对的键都相同，则都将进入同一个reduce中。&lt;/p&gt;
&lt;p&gt;3 reduce类&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;    public static class Reducer extends TableReducer&amp;lt;NullWritable, Text, ImmutableBytesWritable&amp;gt;{
    String time;
    String spName;
    String userID;
    String serverIP;
    String hostName;
    String uploadTraffic;
    String downloadTraffic;
    public void reduce(NullWritable key, Iterable&amp;lt;Text&amp;gt; values, Context context) throws IOException, InterruptedException{
        //逐行将数据写入hbase表中
        for (Text val : values){
            String line = val.toString();
            String[] fields = line.split(&amp;quot;\t&amp;quot;);
            time = fields[0];
            userID = fields[1];
            serverIP = fields[2];
            hostName = fields[3];
            spName = fields[4];
            uploadTraffic = fields[5];
            downloadTraffic = fields[6];
            Put put = new Put(Bytes.toBytes(time));
            put.add(Bytes.toBytes(&amp;quot;colfamily&amp;quot;), Bytes.toBytes(&amp;quot;userID&amp;quot;), Bytes.toBytes(userID));
            put.add(Bytes.toBytes(&amp;quot;colfamily&amp;quot;), Bytes.toBytes(&amp;quot;serverIP&amp;quot;), Bytes.toBytes(serverIP));
            put.add(Bytes.toBytes(&amp;quot;colfamily&amp;quot;), Bytes.toBytes(&amp;quot;hostName&amp;quot;), Bytes.toBytes(hostName));
            put.add(Bytes.toBytes(&amp;quot;colfamily&amp;quot;), Bytes.toBytes(&amp;quot;spName&amp;quot;), Bytes.toBytes(spName));
            put.add(Bytes.toBytes(&amp;quot;colfamily&amp;quot;), Bytes.toBytes(&amp;quot;uploadTraffic&amp;quot;), Bytes.toBytes(uploadTraffic));
            put.add(Bytes.toBytes(&amp;quot;colfamily&amp;quot;), Bytes.toBytes(&amp;quot;downloadTraffic&amp;quot;), Bytes.toBytes(downloadTraffic));
            context.write(null, put);
        }
    }
}
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;整体思路很简单，values是一个数组，每个数据等效为一行数据，因此逐行遍历输出到habse表中即可。这里需要注意的是，hbase表输入数据必须通过对象put，因为需要把输出的值设置为put类型的对象。&lt;/p&gt;
&lt;p&gt;当然，要正常运行，不同于平常的mapreduce的函数是，主函数中需要设置zookeeper,并且必须初始化habse表类型的的reduce类：&lt;/p&gt;
&lt;p&gt;主函数，如下：&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;    public static void main(String[] args) throws IOException, InterruptedException, ClassNotFoundException {
    // TODO Auto-generated method stub
    String[] cfs = {&amp;quot;colfamily&amp;quot;};
    //配置zookeeper
    config.set(&amp;quot;hbase.zookeeper.quorum&amp;quot;, &amp;quot;Master&amp;quot;);
    //创建表
    createTable(targetTable, cfs);
    Job job = new Job(config, &amp;quot;InputHBase&amp;quot;);
    job.setJarByClass(hdfs2hbase.class);
    job.setMapperClass(InputMapper.class);

    TableMapReduceUtil.initTableReducerJob(
            targetTable,        
            Reducer.class,    
            job);
    FileInputFormat.addInputPath(job, new Path(args[0]));
    job.setOutputKeyClass(NullWritable.class);
    job.setOutputValueClass(Text.class);
    boolean b = job.waitForCompletion(true);
    if(!b){
        throw new IOException(&amp;quot;error&amp;quot;);
    }
}
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;strong&gt;实验过程&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;先把代码打成jar包（如Hbase_Input.jar），然后直接在利用命令hadoop jar Hbase_Input.jar /user/yansiyu/hbase/access.log`执行即可，但是注意执行时候后面要加上输入文件的位置:&lt;/p&gt;
&lt;p&gt;先执行jar包：&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;img alt="3" src="http://gitlab.local/course/course_hadoop/uploads/364bb8e4c35cc21778bd0f323ccbe41a/3.png" /&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;查看建立的表和导入的数据&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;img alt="4" src="http://gitlab.local/course/course_hadoop/uploads/78b6e43c161dd17a055a83a89af60114/4.png" /&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;发现表mapreduce_yansiyu已经建立，并且与原来的access.log中数据进行对比，hbase表中数据导入成功。&lt;/p&gt;
&lt;h4&gt;&lt;strong&gt;结语&lt;/strong&gt;&lt;/h4&gt;
&lt;p&gt;总体来说，hbase shell或是api操作还是比较简答的，但是究其原理较为复杂，理解它却更显重要。&lt;/p&gt;
&lt;p&gt;通过这一学期学习---老师讲解与自己练习，从hadoop最基本的原理，mapreduce程序的书写，到hive、pig和hbase的学习与操作，让我对基于hadoop的大数据处理技术有了基本的了解，深知这也只是入门的皮毛而已。在今后的学习中，能够深入其本质，拓展更宽的知识，并能项目或工作结合起来，也许是进一步学习的目标吧。&lt;/p&gt;</summary><category term="HBase MapReduce"></category></entry><entry><title>HBase &amp; HBase shell</title><link href="http://ysywh.github.io/HBase%20&amp;%20HBase%20shell.html" rel="alternate"></link><updated>2015-08-30T17:30:00+08:00</updated><author><name>ysy</name></author><id>tag:ysywh.github.io,2015-06-01:HBase &amp; HBase shell.html</id><summary type="html">&lt;p&gt;&lt;/br&gt;&lt;/p&gt;
&lt;h3&gt;&lt;strong&gt;前言&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;HBase源于Google的BigData，是面向列的、基于HDFS、高性能的分布式数据库系统。面向海量存储和互联网应用。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;面向列：解决稀疏问题，即众多的列，但并非每列都有数据，且经常只访问很少的列&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;基于HDFS：解决海量问题，即低成本可扩展地处理以十亿为单位的数据表&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;高性能：解决高速问题，即高吞吐量和高并发&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;hr /&gt;
&lt;h3&gt;&lt;strong&gt;一 Hbase 与 RRBMS对比&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;RDBMS是基于关系型数据库，而HBase基于列模式的映射数据库，是Nosql模式。Hbase表示为简单的键-值对，简化了关系型数据库。用处不同。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;数据类型：Hbase只有简单的字符串类型，即只保存字符串。关系型数据库数据类型很丰富。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;数据操作：插入、查询、删除和清空等操作，不像关系型数据库表与表之间有关系，所以无表与表之间的关联操作，因为其实也不需要。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;存储模式：Hbase基于列存储,则有些列字段值可以无，且不同列族存储的文件是分离的。&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;除此之外，Hbase更新数据时候会保留一定数量的版本。&lt;/p&gt;
&lt;p&gt;&lt;/br&gt;&lt;/p&gt;
&lt;h3&gt;&lt;strong&gt;二 系统框架&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;构成：Client,Zookeeper，HMaster,HRegion,HRegionServer。&lt;/p&gt;
&lt;p&gt;系统架构图：&lt;/p&gt;
&lt;p&gt;&lt;center&gt;&lt;img alt="Hbase系统构架" src="http://7xnkv8.com1.z0.glb.clouddn.com/Hbase系统构架.png" /&gt;&lt;/center&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Zookeeper用于协同管理，存储-ROOT-表和HMater的地址。HRegionServer也会注册到Zookeeper，方便HMaster感知其健康状态。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;HMaster负责table和region的管理工作。可以启动多个进行保障。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;每个HRegionServer管理多个Hregion,一张表会划分后保存到不同Hregion上，Region由多个HStore组成，每个Hstore对应table中的一个列族。而Hstore由MemStore（用户写入数据）和Hfile组成。&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;即存储为：table  →  Region  →  Store  →  HFile  → Block  →  HDFS File&lt;/p&gt;
&lt;p&gt;&lt;/br&gt;&lt;/p&gt;
&lt;h3&gt;&lt;strong&gt;三 Hbase shell简单操作&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;在终端中输入 &lt;code&gt;hbase shell&lt;/code&gt;进入hbase的CLI模式：&lt;/p&gt;
&lt;p&gt;1 创建表，表名后面必须写列族名称等：&lt;/p&gt;
&lt;p&gt;&lt;code&gt;create 'ysy','colfamily_1','colfamily_2'&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;2 查看表：&lt;/p&gt;
&lt;p&gt;&lt;code&gt;describe 'ysy'&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;3 删除表，先disable表后drop删除：&lt;/p&gt;
&lt;p&gt;&lt;code&gt;disable 'ysy'&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;&lt;code&gt;drop 'ysy'&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;&lt;/br&gt;&lt;/p&gt;
&lt;h3&gt;&lt;strong&gt;四 将hadoop文件系统的文件导入到Hbase表中&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;分两步做1）先生成hfile 2）修改生成的hfile文件所在目录的权限 3）然后将hfile导入到hbase中&lt;/p&gt;
&lt;p&gt;在终端中输入（并非habse的CLI中）：&lt;/p&gt;
&lt;p&gt;1 生成hfile文件：&lt;/p&gt;
&lt;p&gt;&lt;code&gt;hbase org.apache.hadoop.hbase.mapreduce.ImportTsv -Dimporttsv.columns=HBASE_ROW_KEY,colfamily:userID,colfamily:serverIP,colfamily:hostName,colfamily:spName,colfamily:uploadTraffic,colfamily:downloadTraffic -Dimporttsv.bulk.output=/hbase/hfile/access_yansiyu access_yansiyu /yansiyu/hbase/access.log&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;其中，hfileout_yansiyu是将生成的hfile文件目录，access.log是输入文件，这两个文件地址是hdfs文件系统上，access_yansiyu是table名字。colfamily是列族名字，这里只有这一个列族，且冒号后是列名。&lt;/p&gt;
&lt;p&gt;2 修改权限&lt;/p&gt;
&lt;p&gt;由于生成的hfile文件目录权限，否则有时候下一步会执行中进入休止等待状态。&lt;/p&gt;
&lt;p&gt;&lt;code&gt;hadoop fs -chmod -R u+x /hbase/hfile/access_yansiyu&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;其中，目录&lt;code&gt;/hbase/hfile&lt;/code&gt;的权限也也要事先修改好。&lt;/p&gt;
&lt;p&gt;3 hfile导入hbase中：&lt;/p&gt;
&lt;p&gt;&lt;code&gt;hbase org.apache.hadoop.hbase.mapreduce.LoadIncrementalHFiles /hbase/hfile/hfileout_yansiyu access_yansiyu&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;这时候，则已经导入到table中，进入hbase的CLI模式，在table中查看：&lt;/p&gt;
&lt;p&gt;&lt;code&gt;scan 'access_yansiyu'&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;&lt;center&gt;&lt;img alt="mapreduce" src="http://7xnkv8.com1.z0.glb.clouddn.com/mapreduce.png" /&gt;&lt;/center&gt;&lt;/p&gt;
&lt;p&gt;底层转化为Mapreduce执行，可以看到执行成功&lt;/p&gt;
&lt;p&gt;&lt;center&gt;&lt;img alt="scan_table" src="http://7xnkv8.com1.z0.glb.clouddn.com/scan_table.png" /&gt;&lt;/center&gt;&lt;/p&gt;
&lt;p&gt;在表中查看，可以看到内容成功导入。&lt;/p&gt;
&lt;p&gt;&lt;center&gt;&lt;img alt="hue" src="http://7xnkv8.com1.z0.glb.clouddn.com/hue.png" /&gt;&lt;/center&gt;&lt;/p&gt;
&lt;p&gt;还可以在网页的hue中查看。实验成功。&lt;/p&gt;
&lt;p&gt;&lt;/br&gt;&lt;/p&gt;
&lt;h3&gt;&lt;strong&gt;后记&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;这里记录两个需要特别注意的地方：&lt;/p&gt;
&lt;p&gt;1 导入数据的命令要在linux终端中输入，而不是hbase的CLI模式中&lt;/p&gt;
&lt;p&gt;2 生成的hfile文件所在的整体目录权限要进行修改，否则下一步导入会因为权限问题而休止等待。&lt;/p&gt;</summary><category term="HBase"></category></entry><entry><title>设计模式——相关计数pairs、stripes实现</title><link href="http://ysywh.github.io/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F%E2%80%94%E2%80%94%E7%9B%B8%E5%85%B3%E8%AE%A1%E6%95%B0pairs%E3%80%81stripes%E5%AE%9E%E7%8E%B0.html" rel="alternate"></link><updated>2015-08-30T17:30:00+08:00</updated><author><name>ysy</name></author><id>tag:ysywh.github.io,2015-05-28:设计模式——相关计数pairs、stripes实现.html</id><summary type="html">&lt;h4&gt;&lt;strong&gt;前言&lt;/strong&gt;&lt;/h4&gt;
&lt;p&gt;&lt;strong&gt;设计模式&lt;/strong&gt;对于以后写代码很重要，就像现成的模板一样，可以在解决具体问题时候套用。简单的设计模式有计数、分类、过滤出来、排序、去重计数和相关计数，还有相关计数。&lt;/p&gt;
&lt;p&gt;这里主要对“相关计数”这种设计模式进行Mapreduce分析与实现。实现方法主要有两种，pairs和stripes。效率有很大不同，尤其数据量和维度增大时候，stripes的性能就体现的更突出。&lt;/p&gt;
&lt;p&gt;这种设计模式实际应用很多，主要是同现矩阵的建立，比如词库的同现矩阵建立，即某场景下，统计出任意两个词语出现的次数（或者概率）；再比如在超市，会分析大量顾客的购买记录得出商品之间的关系，因为有时候顾客买了这个就会买两一个，因此要通过挖掘这种关系来决定商品的摆放位置。这项生活中的例子，都用到了相关计数这种设计模式，因此其在数据挖掘等领域有重要作用。&lt;/p&gt;
&lt;p&gt;&lt;/br&gt;
&lt;strong&gt;实验目标&lt;/strong&gt;&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;2,3,1,4,5,2,3
1,2,5,2
4,5
1,3,4,1
3,1
4,2,1,5,5,3
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;上面数据是待统计数据，可以理解为：每个编号代表一种商品id,每一行数据为一个人一次购买记录，则有6人购买，即有6行购买记录，现在统计每对商品出现的总次数（对于同一次购买记录中相同物品不进行统计）。如对于（2,1),应该为2+2+0+0+0+1=5.&lt;/p&gt;
&lt;p&gt;&lt;/br&gt;&lt;/p&gt;
&lt;h3&gt;&lt;strong&gt;一 pairs的实现&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;代码逻辑&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt; &lt;span class="k"&gt;class&lt;/span&gt; &lt;span class="n"&gt;Mapper&lt;/span&gt;
    &lt;span class="n"&gt;method&lt;/span&gt; &lt;span class="n"&gt;Map&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;null&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;items&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;i2&lt;/span&gt;&lt;span class="p"&gt;,...]&lt;/span&gt; &lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;all&lt;/span&gt; &lt;span class="n"&gt;item&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="k"&gt;in&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;i2&lt;/span&gt;&lt;span class="p"&gt;,...]&lt;/span&gt;
            &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;all&lt;/span&gt; &lt;span class="n"&gt;item&lt;/span&gt; &lt;span class="n"&gt;j&lt;/span&gt; &lt;span class="k"&gt;in&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;i2&lt;/span&gt;&lt;span class="p"&gt;,...]&lt;/span&gt;
                &lt;span class="n"&gt;Emit&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;pair&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="n"&gt;j&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;count&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;  &lt;span class="c1"&gt;//出现一次发送一次计数&lt;/span&gt;

 &lt;span class="k"&gt;class&lt;/span&gt; &lt;span class="n"&gt;Reducer&lt;/span&gt;
    &lt;span class="n"&gt;method&lt;/span&gt; &lt;span class="n"&gt;Reduce&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;pair&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="n"&gt;j&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;counts&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;c1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;c2&lt;/span&gt;&lt;span class="p"&gt;,...])&lt;/span&gt;
        &lt;span class="n"&gt;s&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;sum&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="n"&gt;c1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;c2&lt;/span&gt;&lt;span class="p"&gt;,...])&lt;/span&gt;               &lt;span class="c1"&gt;//统计出现的次数&lt;/span&gt;
        &lt;span class="n"&gt;Emit&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;pair&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="n"&gt;j&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;count&lt;/span&gt; &lt;span class="n"&gt;s&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;分析：pairs的思想比较简单，就是在pair中统计出每一对，并且计为1，传到reduce中进行加和得出每一对出现的次数。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;代码&lt;/strong&gt;：&lt;/p&gt;
&lt;p&gt;Map部分：&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;public static class Map extends Mapper&amp;lt;Object,Text,Text,IntWritable&amp;gt;{
    protected void map(Object key1, Text value1,Context context) throws IOException, InterruptedException {
        String[] goods=new String[6];
        goods=value1.toString().trim().split(&amp;quot;,&amp;quot;);
        for(int i=0 ; i &amp;lt; goods.length-1;i++){
            for (int j =i+1 ; j&amp;lt;goods.length;j++){
                if(goods[i].equals(goods[j]))continue;
                if(Integer.valueOf(goods[i]) &amp;lt; Integer.valueOf(goods[j])){
                    context.write(new Text(&amp;quot;(&amp;quot;+goods[i]+&amp;quot;,&amp;quot;+goods[j]+&amp;quot;)&amp;quot;), new IntWritable(1));
                }
                else{
                    context.write(new Text(&amp;quot;(&amp;quot;+goods[j]+&amp;quot;,&amp;quot;+goods[i]+&amp;quot;)&amp;quot;), new IntWritable(1));
                }
            }
        }

    }
}
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Reduce部分：&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;public static class Reduce extends Reducer&amp;lt;Text,IntWritable,Text,Text&amp;gt;{

    private static Text result = new Text(); 
    public void reduce(Text key,Iterable&amp;lt;IntWritable&amp;gt; values,Context context) throws IOException,InterruptedException{ 
        int sum = 0;
        //对获取的&amp;lt;key,value-list&amp;gt;计算value的和  
        for(IntWritable val:values){  
            sum++;
        }  
        String sums = String.valueOf(sum);
        result.set(sums); 
        context.write(key, result);  
    }
}
&lt;/pre&gt;&lt;/div&gt;


&lt;h3&gt;&lt;strong&gt;二 stripes的实现&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;代码逻辑：&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt; &lt;span class="k"&gt;class&lt;/span&gt; &lt;span class="n"&gt;Mapper&lt;/span&gt;
     &lt;span class="n"&gt;method&lt;/span&gt; &lt;span class="n"&gt;Map&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;null&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;items&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;i2&lt;/span&gt;&lt;span class="p"&gt;,...]&lt;/span&gt; &lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;all&lt;/span&gt; &lt;span class="n"&gt;item&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="k"&gt;in&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;i2&lt;/span&gt;&lt;span class="p"&gt;,...]&lt;/span&gt;
             &lt;span class="n"&gt;H&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;new&lt;/span&gt; &lt;span class="nl"&gt;AssociativeArray&lt;/span&gt; &lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;item&lt;/span&gt; &lt;span class="o"&gt;-&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;counter&lt;/span&gt;
             &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;all&lt;/span&gt; &lt;span class="n"&gt;item&lt;/span&gt; &lt;span class="n"&gt;j&lt;/span&gt; &lt;span class="k"&gt;in&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;i2&lt;/span&gt;&lt;span class="p"&gt;,...]&lt;/span&gt;
                &lt;span class="n"&gt;H&lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="n"&gt;j&lt;/span&gt;&lt;span class="p"&gt;}&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;H&lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="n"&gt;j&lt;/span&gt;&lt;span class="p"&gt;}&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;               &lt;span class="c1"&gt;//统计和item i同时出现的单词的计数&lt;/span&gt;
             &lt;span class="n"&gt;Emit&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;item&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;stripe&lt;/span&gt; &lt;span class="n"&gt;H&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

 &lt;span class="k"&gt;class&lt;/span&gt; &lt;span class="n"&gt;Reducer&lt;/span&gt;
    &lt;span class="n"&gt;method&lt;/span&gt; &lt;span class="n"&gt;Reduce&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;item&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;stripes&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;H1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;H2&lt;/span&gt;&lt;span class="p"&gt;,...])&lt;/span&gt;
        &lt;span class="n"&gt;H&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;new&lt;/span&gt; &lt;span class="nl"&gt;AssociativeArray&lt;/span&gt; &lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;item&lt;/span&gt; &lt;span class="o"&gt;-&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;counter&lt;/span&gt;
        &lt;span class="n"&gt;H&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;merge&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;sum&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;H1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;H2&lt;/span&gt;&lt;span class="p"&gt;,...]&lt;/span&gt; &lt;span class="p"&gt;)&lt;/span&gt;   &lt;span class="c1"&gt;//按元素进行统计Element-wise sum&lt;/span&gt;
        &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;all&lt;/span&gt; &lt;span class="n"&gt;item&lt;/span&gt; &lt;span class="n"&gt;j&lt;/span&gt; &lt;span class="k"&gt;in&lt;/span&gt; &lt;span class="n"&gt;H&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;keys&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
            &lt;span class="n"&gt;Emit&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;pair&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="n"&gt;j&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;H&lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="n"&gt;j&lt;/span&gt;&lt;span class="p"&gt;})&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;分析：stripes较为复杂，每次map输出的键不再是pair，而是一个，值为一个hashmap（字典型），字典的键不同于map输出键，字典的值是字典的键出现的次数。这样，如第一行输出(2，{3：2，1：1,4:1,5:1,2:1});(3，{1:1,4:1,5:1,2:1,3:1}）,(1,{4:1,5:1,2:1,3:1}),(4,{5:1,2:1,3:1}),(5,{2:1,3:1}),(2,{3:1})。
在reduce中，对所有输入“字典数据”进行统计（Element-wise sum)，即点加和，对应元素位置的值求和。这样每一个reduce输出，就可以得到一个元素和其他每个元素共同出现的次数。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;代码&lt;/strong&gt;：&lt;/p&gt;
&lt;p&gt;Map:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;public static class Map extends Mapper&amp;lt;Object,Text,Text,MapWritable&amp;gt;{
    protected void map(Object key1, Text value1,Context context) throws IOException, InterruptedException {
        String[] goods=new String[6];
        goods=value1.toString().trim().split(&amp;quot;,&amp;quot;);
        for(int i=0 ; i &amp;lt; goods.length-1;i++){
            MapWritable map = new MapWritable(); 
            for (int j =i+1 ; j&amp;lt;goods.length;j++){
                if(goods[i].equals(goods[j]))continue;
                if(map.get(new Text(goods[j])) == null){
                    map.put(new Text(goods[j]), new IntWritable(1));
                }
                else{
                     IntWritable val = (IntWritable) map.get(new Text(goods[j]));
                     IntWritable temp = new IntWritable( val.get() + 1 );
                     map.put(new Text(goods[j]),temp);
                }
            }
            if (!map.isEmpty()) {
                context.write(new Text(goods[i]), map);
            }
       }
    }
}
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Reduce::&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;public static class Reduce extends Reducer&amp;lt;Text,MapWritable,Text,Text&amp;gt;{
    public void reduce(Text key,Iterable&amp;lt;MapWritable&amp;gt; values,Context context) throws IOException,InterruptedException{ 
        HashMap&amp;lt;String , Integer&amp;gt; map = new HashMap&amp;lt;String , Integer&amp;gt;(); 
        for (MapWritable val:values){  
            for(Writable map_key : val.keySet()){
                if(map.get(map_key.toString()) == null){
                    map.put(map_key.toString(), Integer.valueOf(val.get(map_key).toString()));
                }
                else{
                    Integer temp = map.get(map_key.toString())+Integer.valueOf(val.get(map_key).toString());
                    map.put(map_key.toString(),temp);
                }
            }
        }  
        for(String map_key : map.keySet()){
             String key_pair = &amp;quot;(&amp;quot;+key.toString()+&amp;quot;,&amp;quot;+map_key+&amp;quot;)&amp;quot;;
             context.write(new Text(key_pair),new Text(map.get(map_key).toString()));
        } 
    }
}
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;这里需要注意一下，map输出的键值对中的值采用MapWritable类型，代码思路和伪代码相同。&lt;/p&gt;
&lt;h3&gt;&lt;strong&gt;三 实验结果&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;pairs（左），stripes（右）:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;center&gt;&lt;img alt="pairs" src="http://7xnkv8.com1.z0.glb.clouddn.com/pairs.png" /&gt; 
&lt;img alt="stripes" src="http://7xnkv8.com1.z0.glb.clouddn.com/stripes.png" /&gt;&lt;/center&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;在pairs结果中，如（1,2），在实验数据计数可以发现，的确为5次，在stripes中，由于代码没有进一步对反序输出做加和，则结果应该是(1,2)和（2,1）之和，为3+2=5，结果与pairs统计相同，这不影响算法的整体思路，因此成功实现了pairs和stripes的代码实现。&lt;/p&gt;
&lt;p&gt;当然，也可以把stripes中反序和正序作加和，输出就和pairs相同，只需要在map的第二个for循环里，比较i和j元素大小，反序的直接context.write()输出就行。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结语&lt;/strong&gt;：&lt;/p&gt;
&lt;p&gt;stripes和pairs表现出的性能是不同的。pairs的map输出的键由于是pair，因此类型多样，不利于conbiners排序，而stripes方法中map输出键为元素个数，减少了排序，且在combiners有更多的机会执行局部聚集，且高效利用内存。总的来说，在唯维度和数据很大的情况下，stripes性能更优。&lt;/p&gt;
&lt;p&gt;此外，&lt;strong&gt;特别注意&lt;/strong&gt;stripes算法中，map输出为MapWritable类型，不能使用MapHash类型中作为输出输入类型的键值对类型（否则会出现空指针异常错误），可选类型见api库的org.apache.hadoop.io，里面还罗列了许多继承了writeble的数据类型。&lt;/p&gt;</summary><category term="Mapreduce 数据结构"></category></entry><entry><title>Docker搭建Hadoop集群环境和Wordcount实验</title><link href="http://ysywh.github.io/Docker%E6%90%AD%E5%BB%BAHadoop%E9%9B%86%E7%BE%A4%E7%8E%AF%E5%A2%83%E5%92%8CWordcount%E5%AE%9E%E9%AA%8C.html" rel="alternate"></link><updated>2015-08-19T17:30:00+08:00</updated><author><name>ysy</name></author><id>tag:ysywh.github.io,2015-03-11:Docker搭建Hadoop集群环境和Wordcount实验.html</id><summary type="html">&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;利用docker搭建Hadoop环境，并且完成Mapreduce中Word count实验  &lt;/p&gt;
&lt;hr /&gt;
&lt;h4&gt;一：实验目标&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;学习docker基本命令&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;搭建只有一个master和两个slave的Hadoop集群环境&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;进行wordcount实验,理解mapreduce和hdfs等工作机制&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;二：docker基本命令&lt;/h4&gt;
&lt;p&gt;docket是目前较为流行的虚拟化技术，可以方便轻松的在Linux中建立容器，达到虚拟化需求。&lt;/p&gt;
&lt;p&gt;基本操作命令如下，更多参考&lt;a href="!http://dockerpool.com/static/books/docker_practice/index.html"&gt;这里&lt;/a&gt;。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;启动容器：&lt;code&gt;docker run 镜像名&lt;/code&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;暂停容器：&lt;code&gt;docker stop 容器名&lt;/code&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;重启开启容器：&lt;code&gt;docker start 容器名&lt;/code&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;进入容器：&lt;code&gt;docker attach 容器名&lt;/code&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;三：Hadoop集群环境搭建&lt;/h4&gt;
&lt;p&gt;1 Hadoop集群分为一个master和两个slave，因此依次用docker开启相应容器,其中需要做端口映射：&lt;/p&gt;
&lt;p&gt;&lt;img alt="1" src="http://gitlab.local/uploads/course/course_hadoop/955c4756f3/1.png" /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="2" src="http://gitlab.local/uploads/course/course_hadoop/7ce0511f3b/2.png" /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="3" src="http://gitlab.local/uploads/course/course_hadoop/3ef073ba31/3.png" /&gt;&lt;/p&gt;
&lt;p&gt;2 利用命令编辑： &lt;code&gt;vi /etc/hosts&lt;/code&gt;,加入master和两个slave的容器号和ip。之后需要 &lt;strong&gt;开启sshd服务&lt;/strong&gt; ，使其生效：' service  sshd start '&lt;/p&gt;
&lt;p&gt;&lt;img alt="4" src="http://gitlab.local/uploads/course/course_hadoop/d29157c596/4.png" /&gt;&lt;/p&gt;
&lt;p&gt;3 分别登陆master和slave，利用命令chkconfig命令查看服务&lt;/p&gt;
&lt;p&gt;&lt;img alt="5" src="http://gitlab.local/uploads/course/course_hadoop/83312aa0f6/5.png" /&gt;&lt;/p&gt;
&lt;p&gt;4 master上开启相应服务：&lt;/p&gt;
&lt;p&gt;开启Hadoop服务：service hadoop-hdfs-namenode start&lt;/p&gt;
&lt;p&gt;开启mapreduce服务：service hadoop-yarn-resourcemanager start&lt;/p&gt;
&lt;p&gt;开启任务记录的服务：service hadoop-mapreduce-historyserver start&lt;/p&gt;
&lt;p&gt;&lt;img alt="6" src="http://gitlab.local/uploads/course/course_hadoop/6e2e318e55/6.png" /&gt;&lt;/p&gt;
&lt;p&gt;5 在两个slave上分别开启Hadoop服务和mapreduce服务：&lt;/p&gt;
&lt;p&gt;service hadoop-hdfs-datanode start&lt;/p&gt;
&lt;p&gt;service hadoop-yarn-nodemanager start&lt;/p&gt;
&lt;p&gt;&lt;img alt="7" src="http://gitlab.local/uploads/course/course_hadoop/3a2cf8df6a/70.png" /&gt;&lt;/p&gt;
&lt;p&gt;6 三项服务分别可以在web端验证服务是否正确打开。至此，hadoop集群环境搭建完毕&lt;/p&gt;
&lt;p&gt;192.168.59.103:50070&lt;/p&gt;
&lt;p&gt;192.168.59.103:8088&lt;/p&gt;
&lt;p&gt;192.168.59.103:19888&lt;/p&gt;
&lt;p&gt;&lt;img alt="8" src="http://gitlab.local/uploads/course/course_hadoop/2ff2b313c1/8.png" /&gt;&lt;/p&gt;
&lt;h4&gt;三：Word count实验&lt;/h4&gt;
&lt;p&gt;word count 实验就是利用Mapreduce框架，对文本中的单词进行数量统计。&lt;/p&gt;
&lt;p&gt;1 建立一个text文件，写入内容，并put入hdfs文件系统后命名为wordcount.input文件，并查看其内容：&lt;/p&gt;
&lt;p&gt;&lt;img alt="9" src="http://gitlab.local/uploads/course/course_hadoop/47c3b12887/7.png" /&gt;&lt;/p&gt;
&lt;p&gt;2 利用Wordcount脚本进行单词计数的简单实验,经过map和reduce等操作把经过输出到Wordcount.output中:&lt;/p&gt;
&lt;p&gt;&lt;img alt="10" src="http://gitlab.local/uploads/course/course_hadoop/f93f01d942/9.png" /&gt;&lt;/p&gt;
&lt;p&gt;3 登陆web可以观察任务状态，在输出文件中查看Wordcount结果，和预想结果相同，试验成功：&lt;/p&gt;
&lt;p&gt;&lt;img alt="11" src="http://gitlab.local/uploads/course/course_hadoop/7bd9c88362/8.png" /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="12" src="http://gitlab.local/uploads/course/course_hadoop/bccbdf2a9e/10.png" /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="13" src="http://gitlab.local/uploads/course/course_hadoop/9b7e274748/11.png" /&gt;&lt;/p&gt;
&lt;h4&gt;四：心得&lt;/h4&gt;
&lt;p&gt;1 对hdfs文件系统有了基本了解，利用简单的Wordcount实验对map和reduce有了直观认识，深觉此技术的有用性&lt;/p&gt;
&lt;p&gt;2 虽然完成本次试验，但是简单的实验还不能对Hadoop有更深刻的学习，希望今后学习其架构及其源码，多做实验达到学习的目的&lt;/p&gt;
&lt;p&gt;3 最后简单记录一下遇到的问题：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;每次开启容器后，ip可能会发生变化，要记得修改hosts文件，否则会出现 &lt;code&gt;Hadoop fs shell&lt;/code&gt; 无法操作的问题&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;其次，需要关闭安全模式，这样才可以向hdfs文件系统中put内容等操作 &lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;</summary><category term="MapReduce"></category><category term="word count"></category><category term="Docker"></category></entry><entry><title>MapReduce之letter count</title><link href="http://ysywh.github.io/MapReduce%E4%B9%8Bletter%20count.html" rel="alternate"></link><updated>2015-10-01T17:30:00+08:00</updated><author><name>ysy</name></author><id>tag:ysywh.github.io,2015-03-02:MapReduce之letter count.html</id><summary type="html">&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;本节在安装好的Eclipse环境中，尝试使用 MapReduce编程环境，运行letter count代码进行学习验证。letter count 实验就是对一段英文中出现的每个字母进行数量统计，比较基础的练习。&lt;/p&gt;
&lt;hr /&gt;
&lt;p&gt;1 创建新的工程lettercount,并且加载需要的Hadoop库：&lt;/p&gt;
&lt;p&gt;&lt;img alt="1" src="http://gitlab.local/uploads/course/course_hadoop/bf8af6a841/1.png" /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="2" src="http://gitlab.local/uploads/course/course_hadoop/3e50a19c6b/2.png" /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="3" src="http://gitlab.local/uploads/course/course_hadoop/b465f66e7d/3.png" /&gt;&lt;/p&gt;
&lt;p&gt;2 在网上寻找一个mapreduce的Wordcount代码文件，在map类中添加计算letter的代码，并且注释掉计算word的代码，加上对每个字母进行统计的代码，map输出键值对为（字母，one)，在reduce中进行合计，于统计word思路相同：&lt;/p&gt;
&lt;p&gt;&lt;img alt="5" src="http://gitlab.local/uploads/course/course_hadoop/acaaa07b4e/5.png" /&gt;&lt;/p&gt;
&lt;p&gt;3 运行前添加输入和输入文件，输入文件内容是一段英文，经过mapreduce运行计算后会把letter计算结果输出到输出文件中：&lt;/p&gt;
&lt;p&gt;&lt;img alt="4" src="http://gitlab.localuploads/course/course_hadoop/84a81d19fc/4.png" /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="6" src="http://gitlab.local/uploads/course/course_hadoop/9754fce9ef/6.png" /&gt;&lt;/p&gt;
&lt;p&gt;4 通过export输出jar包，然后导入带有Hadoop的Linux系统中，进行实际运行计算：&lt;/p&gt;
&lt;p&gt;&lt;img alt="7" src="http://gitlab.local/uploads/course/course_hadoop/a1c5c158d1/7.png" /&gt;&lt;/p&gt;
&lt;p&gt;5 可以通过对比输入文件内容，比较计算的输出的结果，输出文件为两个：&lt;/p&gt;
&lt;p&gt;&lt;img alt="8" src="http://gitlab.local/uploads/course/course_hadoop/f70bf9a61c/8.png" /&gt;&lt;/p&gt;</summary><category term="mapreduce"></category><category term="letter count"></category></entry></feed>