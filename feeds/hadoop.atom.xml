<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>ysy记录点滴</title><link href="http://ysywh.github.io/" rel="alternate"></link><link href="http://ysywh.github.io/feeds/hadoop.atom.xml" rel="self"></link><id>http://ysywh.github.io/</id><updated>2015-10-01T17:30:00+08:00</updated><entry><title>letter_count</title><link href="http://ysywh.github.io/pages/2015/03/05/create-blog.html" rel="alternate"></link><updated>2015-10-01T17:30:00+08:00</updated><author><name>ysy</name></author><id>tag:ysywh.github.io,2015-03-05:pages/2015/03/05/create-blog.html</id><summary type="html">&lt;h3&gt;MapReduce编程环境的搭建&lt;/h3&gt;
&lt;blockquote&gt;
&lt;p&gt;本节尝试 MapReduce编程环境，安装Eclipse，并且运行lettercount代码进行验证。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h4&gt;一：环境搭建和lettercount实验&lt;/h4&gt;
&lt;h4&gt;1. 根据file中提供的压缩包，在win8上解压安装了eclipse，并且加载Hadoop库和环境变量。&lt;/h4&gt;
&lt;h4&gt;2. 创建新的工程lettercount,并且加载需要的Hadoop库：&lt;/h4&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;img alt="1" src="http://gitlab.local/uploads/course/course_hadoop/bf8af6a841/1.png" /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="2" src="http://gitlab.local/uploads/course/course_hadoop/3e50a19c6b/2.png" /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="3" src="http://gitlab.local/uploads/course/course_hadoop/b465f66e7d/3.png" /&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h4&gt;3. 在网上寻找一个mapreduce的Wordcount代码文件，在map类中添加计算letter的代码，并且注释掉计算word的代码：&lt;/h4&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;img alt="5" src="http://gitlab.local/uploads/course/course_hadoop/acaaa07b4e/5.png" /&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h4&gt;4. 运行前添加输入和输入文件，输入文件内容是一段英文，经过mapreduce运行计算后会把letter计算结果输出到输出文件中：&lt;/h4&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;img alt="4" src="http://gitlab.localuploads/course/course_hadoop/84a81d19fc/4.png" /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="6" src="http://gitlab.local/uploads/course/course_hadoop/9754fce9ef/6.png" /&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h4&gt;5. 通过export输出jar包，然后导入带有Hadoop的Linux系统中，进行实际运行计算：&lt;/h4&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;img alt="7" src="http://gitlab.local/uploads/course/course_hadoop/a1c5c158d1/7.png" /&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h4&gt;6. 可以通过对比输入文件内容，比较计算的输出的结果,输出文件为两个：&lt;/h4&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;img alt="8" src="http://gitlab.local/uploads/course/course_hadoop/f70bf9a61c/8.png" /&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h4&gt;二：后记&lt;/h4&gt;
&lt;p&gt;好久没有Java编程，今天通过简单的修改lettercount又重新熟悉了Java，但还是不娴熟，需要进一步提高学习。&lt;/p&gt;
&lt;p&gt;其次，编程环境搭建在win8上，很不方便，有机会直接搭建在带有hadoop的Linux系统中。&lt;/p&gt;</summary><category term="github pages"></category><category term="pelican"></category></entry><entry><title>docker搭建Hadoop的</title><link href="http://ysywh.github.io/pages/2015/03/01/create-blog.html" rel="alternate"></link><updated>2015-08-19T17:30:00+08:00</updated><author><name>ysy</name></author><id>tag:ysywh.github.io,2015-03-01:pages/2015/03/01/create-blog.html</id><summary type="html">&lt;h3&gt;利用Docker搭建Hadoop集群环境和Wordcount实验&lt;/h3&gt;
&lt;blockquote&gt;
&lt;p&gt;前言：初次接触docker和Hadoop，特此记录实验过程和学习心得&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h4&gt;一：实验目标&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;学习docker基本命令&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;搭建只有一个master和两个slave的Hadoop集群环境&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;进行wordcount实验,理解mapreduce和hdfs等工作机制&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;实验心得&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;二：docker基本命令&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;启动容器：&lt;code&gt;docker run 镜像名&lt;/code&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;暂停容器：&lt;code&gt;docker stop 容器名&lt;/code&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;重启开启容器：&lt;code&gt;docker start 容器名&lt;/code&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;进入容器：&lt;code&gt;docker attach 容器名&lt;/code&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;三：Hadoop集群环境搭建&lt;/h4&gt;
&lt;h5&gt;1. Hadoop集群分为一个master和两个slave，因此依次用docker开启相应容器,其中需要做端口映射：&lt;/h5&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;img alt="1" src="http://10.106.128.234/uploads/course/course_hadoop/955c4756f3/1.png" /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="2" src="http://10.106.128.234/uploads/course/course_hadoop/7ce0511f3b/2.png" /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="3" src="http://10.106.128.234/uploads/course/course_hadoop/3ef073ba31/3.png" /&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h5&gt;2. 利用命令编辑： &lt;code&gt;vi /etc/hosts&lt;/code&gt;,加入master和两个slave的容器号和ip。之后需要 &lt;strong&gt;开启sshd服务&lt;/strong&gt; ，使其生效：' service  sshd start '&lt;/h5&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;img alt="4" src="http://10.106.128.234/uploads/course/course_hadoop/d29157c596/4.png" /&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h5&gt;3. 分别登陆master和slave，利用命令chkconfig命令查看服务&lt;/h5&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;img alt="5" src="http://10.106.128.234/uploads/course/course_hadoop/83312aa0f6/5.png" /&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h5&gt;4. master上开启相应服务：&lt;/h5&gt;
&lt;p&gt;开启Hadoop服务：service hadoop-hdfs-namenode start&lt;/p&gt;
&lt;p&gt;开启mapreduce服务：service hadoop-yarn-resourcemanager start&lt;/p&gt;
&lt;p&gt;开启任务记录的服务：service hadoop-mapreduce-historyserver start&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;img alt="6" src="http://10.106.128.234/uploads/course/course_hadoop/6e2e318e55/6.png" /&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h5&gt;5. 在两个slave上分别开启Hadoop服务和mapreduce服务：&lt;/h5&gt;
&lt;p&gt;service hadoop-hdfs-datanode start&lt;/p&gt;
&lt;p&gt;service hadoop-yarn-nodemanager start&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;img alt="7" src="http://10.106.128.234/uploads/course/course_hadoop/3a2cf8df6a/70.png" /&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h5&gt;6. 三项服务分别可以在web端验证服务是否正确打开。至此，hadoop集群环境搭建完毕&lt;/h5&gt;
&lt;p&gt;192.168.59.103:50070&lt;/p&gt;
&lt;p&gt;192.168.59.103:8088&lt;/p&gt;
&lt;p&gt;192.168.59.103:19888&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;img alt="8" src="http://10.106.128.234/uploads/course/course_hadoop/2ff2b313c1/8.png" /&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h4&gt;三：Wordcount实验&lt;/h4&gt;
&lt;h5&gt;1. 建立一个text文件，写入内容，并put入hdfs文件系统后命名为wordcount.input文件，并查看其内容：&lt;/h5&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;img alt="9" src="http://10.106.128.234/uploads/course/course_hadoop/47c3b12887/7.png" /&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h5&gt;2. 利用Wordcount脚本进行单词计数的简单实验,经过map和reduce等操作把经过输出到Wordcount.output中:&lt;/h5&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;img alt="10" src="http://10.106.128.234/uploads/course/course_hadoop/f93f01d942/9.png" /&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h5&gt;3. 登陆web可以观察任务状态，在输出文件中查看Wordcount结果，和预想结果相同，试验成功：&lt;/h5&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;img alt="11" src="http://10.106.128.234/uploads/course/course_hadoop/7bd9c88362/8.png" /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="12" src="http://10.106.128.234/uploads/course/course_hadoop/bccbdf2a9e/10.png" /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="13" src="http://10.106.128.234/uploads/course/course_hadoop/9b7e274748/11.png" /&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h4&gt;四：实验心得&lt;/h4&gt;
&lt;h5&gt;1. 对hdfs文件系统有了基本了解，利用简单的Wordcount实验对map和reduce有了直观认识，深觉此技术的有用性&lt;/h5&gt;
&lt;h5&gt;2. 虽然完成本次试验，但是简单的实验还不能对Hadoop有更深刻的学习，希望今后学习其架构及其源码，多做实验达到学习的目的&lt;/h5&gt;
&lt;h5&gt;3. 最后简单记录一下遇到的问题：&lt;/h5&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;每次开启容器后，ip可能会发生变化，要记得修改hosts文件，否则会出现 &lt;code&gt;Hadoop fs shell&lt;/code&gt; 无法操作的问题&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;其次，需要关闭安全模式，这样才可以向hdfs文件系统中put内容等操作 &lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;-------2015.3.24&lt;/p&gt;</summary><category term="github pages"></category><category term="pelican"></category></entry></feed>