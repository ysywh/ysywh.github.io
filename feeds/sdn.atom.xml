<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>YsY记录点滴</title><link href="http://ysywh.github.io/" rel="alternate"></link><link href="http://ysywh.github.io/feeds/sdn.atom.xml" rel="self"></link><id>http://ysywh.github.io/</id><updated>2015-11-01T17:30:00+08:00</updated><entry><title>Tunnel技术 之 二层隧道协议</title><link href="http://ysywh.github.io/pages/2015/11/01/Tunnel%E6%8A%80%E6%9C%AF%20%E4%B9%8B%20%E4%BA%8C%E5%B1%82%E9%9A%A7%E9%81%93%E5%8D%8F%E8%AE%AE.html" rel="alternate"></link><updated>2015-11-01T17:30:00+08:00</updated><author><name>ysy</name></author><id>tag:ysywh.github.io,2015-11-01:pages/2015/11/01/Tunnel技术 之 二层隧道协议.html</id><summary type="html">&lt;p&gt;之前从隧道协议引入，说到了而层隧道协议用到的一个基本协议——点到点协议（PPP）。说它是建立在点到点的直连或是逻辑直连的环境下适用的，并且自身具有身份认证、对ppp载荷数据加密等功能。有了这个基础，本文可以切入二层隧道协议主题了，说说两个最常用的二层隧道协议：PPTP和L2TP。&lt;/p&gt;
&lt;h3&gt;PPTP：(Point to Point Tunneling Protocol)&lt;/h3&gt;
&lt;p&gt;PPTP称为点到点的隧道协议。PPTP的载荷协议是PPP，传输协议是IP。换句话说，就是把二层的PPP帧封装在IP包中进行传输，以此完成站点到站点的VPN连接。&lt;/p&gt;
&lt;p&gt;PPTP协议要根据工作阶段分为两种：连接控制和数据传输。PPTP这两个阶段的报文格式也不同。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;1 连接管理报文（RFC2637）&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;报文格式： IP+TCP+PPTP&lt;/p&gt;
&lt;p&gt;把PPTP归类在二层隧道协议里，这是从载客协议角度看的。但从上面报文格式可以看出，PPTP这种协议本身可以算是一种应用层协议。&lt;/p&gt;
&lt;p&gt;在连接管理阶段，主要完成PPTP隧道建立、echo定时保活和最终隧道断开。服务端PPTP的TCP保留报端是1723，隧道建立时候会给客户端和服务端分别分配一个call id（可以类比传输层的端口，不严格），用来标识会话，在封装和解封隧道包时候使用。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;2 数据传输阶段(GRE， RFC2784)&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;报文格式：&lt;/p&gt;
&lt;p&gt;可以看到乘客协议（passenger protocol）是PPP报文，传输协议（transport protocol）是IP报文。当PPTP完成链路连接协商后，就可以利用乘客协议PPP传输数据了，当然要特别注意，在真正数据传输前，是少不了PPP协议建立协商的过程的，这个如PPP那篇博文描述的。&lt;/p&gt;
&lt;p&gt;还需要注意一点，IP报文是不能直接封装一个PPP的二层数据帧的，IP里没有这个功能，因此需要用到GRE协议。GRE是一个通用路由协议，可以封装任意层的协议（其实GRE本身就是一种隧道协议）。GRE协议属于网络层协议，和IP同属于一层，可以封装在IP里。因此，PPP帧需要加通用路由协议（GRE）去封装，然后再通过IP报文进行传输。&lt;/p&gt;
&lt;h3&gt;L2TP（Layer 2 Tunneling Protocol）&lt;/h3&gt;
&lt;p&gt;L2TP被称为第二层隧道协议，它是在L2F协议（cisco提出）上发展而来，和PPTP一样，也是一个二层的隧道协议，也和PPTP一样，L2TP本身也是一个应用层协议。L2TP和PPTP一个很大不同点，就是传输协议(承载协议)不只是IP，所以应用范围更广。&lt;/p&gt;
&lt;p&gt;L2TP根据阶段不同也分为控制包和数据包，但包结构相同，且控制包没有PPP报文这样的载荷。&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;控制包：完成端到端隧道的建立、协商、维护、断开等。其中建立协商阶段也会得到隧- 道id和会话id。L2TP的控制包有重传机制，保障可靠性。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;数据包：就是在L2TP上承载PPP载荷。当然也会包括PPP一系列身份验证、加密等过程。L2TP的数据包无重传机制，但可以通过选择TCP保障可靠性。&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;以下是传输协议为IP+UDP时候的PPTP数据包报文格式：&lt;/p&gt;
&lt;p&gt;当然，L2TP可以进行IPsec加密，达到端到端的加密和身份验证的目的（PPTP也可以在GRE外进行IPsec封装，即GRE over IPsec）：&lt;/p&gt;
&lt;p&gt;个人理解，这比PPP自身的加密和验证更完备，因为把数据部分，即PPP帧这样的二层也进行了验证、加密。&lt;/p&gt;
&lt;p&gt;这里需要提一下，IPsec有两种工作模式，“运输方式“和"隧道模式"（即VPN模式）。这里IPsec加密验证仅仅采用IPsec的“运输方式”的工作模式，即不对外层的IP进行加密。其实，当IPsec工作在隧道模式下，它自身也算是一种三层的隧道协议，会对数据的IP以及IP载荷进行加密，然后重新封装IP头。IPsec会另外开篇讲到。&lt;/p&gt;
&lt;h3&gt;PPTP与L2TP异同点&lt;/h3&gt;
&lt;p&gt;相同点就是载荷都是PPP报文。&lt;/p&gt;
&lt;p&gt;不同点：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;PPTP要求穿越的互联网为IP网络；L2TP只要求隧道媒介可以提供点到点的连接即可。比如L2TP的传输协议可以是IP+UDP(TCP)、帧中继永久虚电路（PVCs）、X2.5虚电路和ATM网络。&lt;/li&gt;
&lt;li&gt;PPTP只能在两端点建立单一隧道；L2TP可以在两端点建立多条隧道，用户可以根据不同质量服务创建不同隧道。&lt;/li&gt;
&lt;li&gt;PPTP不支持隧道验证；L2TP支持隧道验证。但当PPTP或是L2TP与IPsec共同使用，则提供隧道验证。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;2、3理解不到位，但也基本不是他们的区别重点。可以多多指教。&lt;/p&gt;</summary><category term="SDN"></category></entry><entry><title>Tunnel技术</title><link href="http://ysywh.github.io/pages/2015/10/30/Tunnel%E6%8A%80%E6%9C%AF.html" rel="alternate"></link><updated>2015-11-01T17:30:00+08:00</updated><author><name>ysy</name></author><id>tag:ysywh.github.io,2015-10-30:pages/2015/10/30/Tunnel技术.html</id><summary type="html">&lt;p&gt;随着一些虚拟化技术的发展，计算资源、存储资源和网络资源虚拟化也越来越成熟。单说网络虚拟化，就有很多成熟的技术。这里面，不得不说的一个就是隧道技术（Tunnel），虚拟专用网（VPN）的构建就用到Tunnel技术。&lt;/p&gt;
&lt;h3&gt;Tunnel 技术&lt;/h3&gt;
&lt;p&gt;网络隧道技术指一种协议数据包可以承载封装在另一种数据包中传输的技术。被封装承载的协议可称作乘客协议（transport protocol），载体协议可以被称为传输协议（transfer protocol）。为什么要在本来的乘客协议里外面再封装一层传输协议呢？说的简单点，就是用传输协议来穿越（路由寻址）这个适合它穿越的的这个网络，来建立两端的连通。比如两个企业的内网需要通过公共IP网络连通，组成一个虚拟私有网，这其实也VPN的内涵了。&lt;/p&gt;
&lt;p&gt;当然，利用隧道协议进行数据传输，保障数据的安全性和完整性很重要，不同隧道协议里都会有相应的验证、加密机制。&lt;/p&gt;
&lt;p&gt;通过上面我们知道，乘客协议需要封装在传输协议中进行传输，但也不能直接封装吧，往往传输协议（如IP）是不支持直接封装另一个协议（如PPP），因此需要封装协议作为中间过渡桥梁。封装协议是隧道技术的核心，有L2TP、IPsec、MPLS、SSL等多种协议技术。这里，我们根据乘客协议不同，将其分为二层隧道协议（PPTP、L2TP）、三层隧道协议（GRE、IPsec VPN）或高层隧道协议(ssl VPN)等进行讨论。文章宗旨是从协议与其技术的整体认识进行把握，协议的实现细节、报文格式每个字段解释都是一个比较繁琐复杂的部分，具体现查现用即可。&lt;/p&gt;
&lt;p&gt;我们从二层协议说起。但是在二层隧道协议里，如PPTP或是L2TP协议，他们有一个共同特点，就是载荷协议都是PPP帧。因此，这篇博文有必要先对PPP协议有基本了解，顺带说说基于它的PPPoE协议。&lt;/p&gt;
&lt;h3&gt;PPP协议（point-to-point protocol）&lt;/h3&gt;
&lt;p&gt;隧道技术要完成的一个重要内容就是点到点连通，无论是从外部远程连接一个网络构成虚拟专用网（如在外办公者远程连接企业网），还是跨异构网络连接多个特有网络（如企业网和外地分公司网络），都基本实现了一个点到点的连接。&lt;/p&gt;
&lt;p&gt;其中，PPP就是一个点到点的二层协议（和mac是一层），但PPP既然是点到点，那么就必须在点到点的环境下运行，比如两端网线直连，或是逻辑直连（比如通过PPTP、L2TP甚至PPPoE提供点到点的逻辑连接）。&lt;/p&gt;
&lt;p&gt;PPP这种点到点的协议，有一个很大特点就是可以完成&lt;strong&gt;身份认证&lt;/strong&gt;和&lt;strong&gt;载荷加密&lt;/strong&gt;的功能，因此无论是PPPoE还是二层隧道技术（PPTP和L2TP）都基本是利用了这一点。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;PPP报文&lt;/strong&gt;：&lt;/p&gt;
&lt;p&gt;PPP报文比较特殊，虽然有很多字段，但是基本无用：&lt;/p&gt;
&lt;p&gt;由于是点对点，没必本端和对端地址。相对来说，只有“协议”字段和“数据”字段有用，用来指示下一层协议。下一层协议可以是IP，IPX等三层协议；数据就是PPP帧的载荷，比如IP报文。&lt;/p&gt;
&lt;p&gt;很多协议根据工作阶段都可以分为控制管理阶段和数据传输阶段。两个阶段协议报文格式有可能不同、甚至用到其他协议，他们一起可以称为协议簇了。&lt;/p&gt;
&lt;p&gt;PPP协议也不例外，也是一个协议簇。在开始传输数据前，需要进行PPP链路建立、认证、IP报文控制等一系列操作，这些管理操作要用到PPP协议的一些管理协议，他们都是封装在PPP包头之后（就像arp在mac包头之后一样）。&lt;/p&gt;
&lt;p&gt;PPP连接管理阶段用到协议分三种：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;链路控制协议（LCP：Link Control Protocol），主要用来建立、拆除和监控数据链路。&lt;/li&gt;
&lt;li&gt;网络层控制协议簇（NCP：Network Control Protocol）：NCP不是具体的协议，它包括IPCP、IPCPv6、IPXCP协议，主要用来协商在该数据链路上所传输的网络层数据包（如IP包）的格式与类型。&lt;/li&gt;
&lt;li&gt;扩展协议族：这是一组协议，主要用来保障安全性。例如用于网络安全方面的验证的PAP和CHAP。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;交互过程&lt;/strong&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;PPP链路协商建立：LCP用于PPP链路连接和断开时候的控制协议，比如链路建立、拆除，MRU（最大接收单元）协商、安全验证协议的选择（选PAP还是CHAP）、整个PPP通信过程还有echo消息的定时保活、甚至PPP断开等，所以PPP的LCP报文贯穿于整个PPP网络通信开始到结束。&lt;/li&gt;
&lt;li&gt;认证：如果协商具有认证阶段（可以没有），则用安全验证报文（PAP、CHAP）验证客户端的用户名密码等内容。&lt;/li&gt;
&lt;li&gt;网络层协议控制：认证或是LCP链路建立后，PPP的NCP协议报文（如IPCP）会定义网络层数据包的格式、压缩方式、分配客户端源IP地址分配、加密方式等，比如后期PPP报文加密用到的协议（则PPP建立后PPP载荷内容会用协商的加密协议传输）。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;完成以上三个PPP管理控制阶段之后，则可以在PPP载荷中传输报文，如IP报文。当然，通信中定时会有LCP的echo消息保活，最后通信结束会用LCP报文断开连接。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;总结&lt;/strong&gt;：PPP点对点传输，必须在点到点的连接或是逻辑连接环境下运行。主要用于需要身份认证的场景。&lt;/p&gt;
&lt;h3&gt;PPPoE (point-to-point protocol over ethernet)&lt;/h3&gt;
&lt;p&gt;以太网本一旦搭建后，互相可以连通的（至少通过mac），为什么还需要PPPoE能？有这样一个场景，若ISP为了给接入互联网的用户增加计费功能，就必须那些用户接入ISP服务器后来上网的用户进行身份验证，然后计费。这样，就需要PPP的身份认证功能，PPPoE就派上用场。ISP的ADSL就用到PPPoE接入技术。&lt;/p&gt;
&lt;p&gt;PPPoE是二层协议，报文格式简单表示为：mac报文头+PPPoE报文头+PPP报文头+三层报文，显然PPPoE是用到PPP帧的。我们知道，PPP必须是端到端环境下（比如直连，或是逻辑直连），但在以太网中客户端要上网，就需要PPPoE来建立客户端到服务端的端到端逻辑链路（这个在地址发现阶段完成，如下），这时候PPP才能派上用场。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt; 交互过程 &lt;/strong&gt;：
1 Discovery阶段（地址发现阶段）：通过主动广播、交互，从以太网中很多“访问集中器”（相当于上网的出口）中选出一个，即得到选定“访问集中器”的mac，还要分配一个session-id，，标记一次PPP会话，在PPP会话阶段中使用，不能改变。地址发现阶段结束后，就有了以太网端到端的所有信息，然后就可以开始端到端的连接会话。
2 PPP会话阶段（和PPP一样）：PPP的建立端到端连接、认证、三层协议控制（如分Ip协议协商控制）等。之后就可以在PPP载荷中加数据传输了。&lt;/p&gt;
&lt;p&gt;PPPoE实质就是拨号（接入服务本质就是PPP的认证过程）。目前，ADSL场景会用，用modem拨号接入ISP上网、WLAN网也会用、PPPoE还可以解决IP不足问题。&lt;/p&gt;
&lt;p&gt;总结：PPPoE利用PPP的认证功能，完成以太网的网络接入认证。&lt;/p&gt;</summary><category term="SDN"></category></entry><entry><title>Ryu代码分析----openflow消息属性分析</title><link href="http://ysywh.github.io/pages/2015/07/22/Ryu%E4%BB%A3%E7%A0%81%E5%88%86%E6%9E%90----openflow%E6%B6%88%E6%81%AF%E5%B1%9E%E6%80%A7%E5%88%86%E6%9E%90.html" rel="alternate"></link><updated>2015-08-30T17:30:00+08:00</updated><author><name>ysy</name></author><id>tag:ysywh.github.io,2015-07-22:pages/2015/07/22/Ryu代码分析----openflow消息属性分析.html</id><summary type="html">&lt;h3&gt;前言&lt;/h3&gt;
&lt;p&gt;当从交换机接收到openflow消息，我们暂且称为msg， 这个msg实体中有很多属性，分两种：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;msg继承自基类MsgBase的基本属性：datapath，version，msg_type，msg_len，xid，buf 等&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;每种openflow消息自己的属性。比如packetin消息，具有match,data（数据层面的传统数据包的全部内容或是只是报头内容）等属性。 &lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;当我们 openflow消息，需要对消息msg进行解析，甚至是构造openflow消息发送出去。这些消息都是由属性成员构成，因此对其属性进行分析极其重要。&lt;/p&gt;
&lt;p&gt;packetin消息是一个极其重要的openflow消息，它具有如下属性。flowmod属性中也有match属性，还有instruction属性，packet-out中还有actions属性。本位对这些重要属性进行解析。&lt;/p&gt;
&lt;p&gt;Packet-in消息属性：&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;buffer_id     ID assigned by datapath
total_len     Full length of frame
reason        Reason packet is being sent.

              | OFPR_NO_MATCH
              | OFPR_ACTION
              | OFPR_INVALID_TTL
table_id      ID of the table that was looked up
cookie        Cookie of the flow entry that was looked up
match         Instance of ``OFPMatch``
data          Ethernet frame
&lt;/pre&gt;&lt;/div&gt;


&lt;hr /&gt;
&lt;h4&gt;&lt;strong&gt;1 一般属性的解析&lt;/strong&gt;&lt;/h4&gt;
&lt;p&gt;如msg属性buffer_id，datapath可直接访问，如msg.datapath**&lt;/p&gt;
&lt;h4&gt;&lt;strong&gt;2 对于match这个属性解析和反解析（生成）：&lt;/strong&gt;&lt;/h4&gt;
&lt;p&gt;match属性解析主要用在packet-in消息中的属性match的解析，而match生成用于flowmod消息的match构造上。&lt;/p&gt;
&lt;p&gt;用到主要类：OFPMatch(StringifyMixin)类，形成match实体。&lt;/p&gt;
&lt;p&gt;OFPMatch类生成对象实例match，对象match中一个40元组的字典型属性（暂且这么理解，则也可把match对象看做字典型，和字典一样操作）,字典的键如in_port，ipv4_src等。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;解析match：match[键名]获得键值，如msg.match[in_port]得到流表项匹配域的入端口（即数据包入端口）（因为OFPMatch()中定义__getitem__函数，提供下标访问方式，相当于[]重载）&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;生成match：直接用键值形式作为参数调用OFPMatch() 类：如：&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;match = parser.OFPMatch(
in_port=1,
eth_type=0x86dd,
ipv6_src=(&amp;#39;2001:db8:bd05:1d2:288a:1fc0:1:10ee&amp;#39;,&amp;#39;ffff:ffff:ffff:ffff::&amp;#39;),
ipv6_dst=&amp;#39;2001:db8:bd05:1d2:288a:1fc0:1:10ee&amp;#39;)
if &amp;#39;ipv6_src&amp;#39; in match:
print match[&amp;#39;ipv6_src&amp;#39;]
&lt;/pre&gt;&lt;/div&gt;


&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;注明：parser是相应版本的解析包名，最好用parser=datapath.ofproto_parser，得到解析包名，如ofproto_v1_3_parser。&lt;/p&gt;
&lt;p&gt;当然，还有OFPMatchField匹配域，掩码问题...先不分析。&lt;/p&gt;
&lt;h4&gt;&lt;strong&gt;3 actions 、 instruction&lt;/strong&gt;&lt;/h4&gt;
&lt;p&gt;作用：当进入交换机封包满足match匹配则执行指令instruction，有些指令中含有动作action(每条流表项可以有多个action，组成action list，因此是数组型)。instruction是flowmod消息下发时候包含的，当然，有的instruction中含有action。而packet-out消息直接含有action属性。&lt;/p&gt;
&lt;p&gt;对于flowmod，action是包含于instruction中的；而对于packet-out,则只有actions。因此，我们这里先对action进行分析，后再分析instruction。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;actions&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;用法：调用action类实例化action。&lt;/p&gt;
&lt;p&gt;举例：以OFPActionOutput为例，调用OFPActionOutput类可生成action对象。 &lt;/p&gt;
&lt;p&gt;OFPActionOutput(OFPAction):用来生成“指定从交换机出口发包”action对象的类（出口动作）。主要用于packout和flowmod的action。&lt;/p&gt;
&lt;p&gt;定义：&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;def __init__(self, port, max_len=ofproto.OFPCML_MAX,type_=None, len_=None):
    super(OFPActionOutput, self).__init__()
    self.port = port
    self.max_len = max_len。
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;参数：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;max_len参数针对发给控制器的packin包，当为0时，则不含传统数据包负载；为OFPCML_NO_BUFFER时，则所有封包加入packin中上传给控制器。缺省值为OFPCML_MAX，表示发给控制器最大包长。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Port是包出端口。可以是指定端口号（如1,2等），还可以是：&lt;/p&gt;
&lt;p&gt;OFPP_IN_PORT，OFPP_TABLE ，OFPP_NORMAL，OFPP_FLOOD， OFPP_ALL，OFPP_CONTROLLER(发给控制器),OFPP_LOCAL，OFPP_ANY具体含义见定义（如ofproto_v1_3.py中）。&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;其他类：除了OFPActionOutput，还有OFPActionGroup(OFPAction) OFPActionSetQueue(OFPAction)等类，他们都继承了OFPAction类。用法同OFPActionOutput类生成action对象一样。&lt;/p&gt;
&lt;p&gt;注意，actions可以有很多，因此actions是一个数组型，包含多个action实体。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;instruction(指令)&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;说明：当匹配一条流表项后，则执行流表项的指令集（数组型，可包含多个指令），有的指令中又可以包含多个action，ryu实现了1.3中7种的6种类型，由四个类实现（其中OFPInstructionActions实现了三种类型）。&lt;/p&gt;
&lt;p&gt;用法：需要调用instruction相关类生成。主要有四种：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;OFPInstructionGotoTable&lt;/li&gt;
&lt;li&gt;OFPInstructionWriteMetadata&lt;/li&gt;
&lt;li&gt;OFPInstructionActions&lt;/li&gt;
&lt;li&gt;OFPInstructionMeter&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;他们都继承OFPInstruction类，例如OFPInstructionActions：定义如下：&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="k"&gt;class&lt;/span&gt; &lt;span class="n"&gt;OFPInstructionActions&lt;/span&gt;(&lt;span class="n"&gt;OFPInstruction&lt;/span&gt;):
    &lt;span class="n"&gt;def&lt;/span&gt; &lt;span class="n"&gt;__init__&lt;/span&gt;(&lt;span class="k"&gt;self&lt;/span&gt;, &lt;span class="n"&gt;type_&lt;/span&gt;, &lt;span class="n"&gt;actions&lt;/span&gt;=&lt;span class="n"&gt;None&lt;/span&gt;, &lt;span class="n"&gt;len_&lt;/span&gt;=&lt;span class="n"&gt;None&lt;/span&gt;):
    ...
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;参数：type_表示匹配后在执行action前的执行动作（不同于action，比如OFPActionOutput表示包出口动作。type_可以是三种：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;OFPIT_APPLY_ACTIONS(立即执行action list中所有action)；&lt;/li&gt;
&lt;li&gt;OFPIT_WRITE_ACTIONS(把action list中多个action放入 action set中，等所有流表处理完一 次性处理action set中所有action，只支持单流表无意义)；&lt;/li&gt;
&lt;li&gt;OFPIT_CLEAR_ACTIONS（清除前面流表所产生action set中所有action，单流表无意义）&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;actions即为上面的action对象。&lt;/p&gt;
&lt;p&gt;因此，OFPInstructionGotoTable（table转移，在只是支持单流表的交换机无意义），OFPInstructionWriteMetadata， OFPInstructionMeter用法同理。&lt;/p&gt;
&lt;h3&gt;4 data属性&lt;/h3&gt;
&lt;p&gt;data是msg中一个很重要的属性，当数据层面的交换机接收到传统包，data就是这个传统包的所有内容或是报头内容。&lt;/p&gt;
&lt;p&gt;对data的解析出现在packet-in中，当packetout时候，有时候会用到data的封装。&lt;/p&gt;
&lt;p&gt;主要类：class Packet(object):&lt;/p&gt;
&lt;p&gt;&lt;code&gt;def __init__(self, data=None, protocols=None, parse_cls=ethernet.ethernet)&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;Packet()类用来对传统包编解码（填入包头和解析包头，如ip包头等）&lt;/p&gt;
&lt;p&gt;位置：\ryu\lib\packet\packet.py&lt;/p&gt;
&lt;p&gt;类方法：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;add_protocol(self, proto)：如给控制器下发包中传统数据包头添加协议字段（如ip）。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;get_protocols(self, protocol)：解析传统数据包包头协议字段，返回协议对象的列表&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;get_protocol(self, protocol)：同上，不同是返回与protocol匹配的第一个协议对象
用法：&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;封包解析&lt;/p&gt;
&lt;p&gt;对于packIn的msg，msg属性data是传统以太帧内容（即进入交换机的封包），但它是bit字符串，需要以data为参数，通过调用类Packet(object)生成对象pkt来使用。&lt;/p&gt;
&lt;p&gt;pkt利用类Packet方法get_protocols()或get_protocol()得到协议字段值（如Ip包头，mac头等）    &lt;/p&gt;
&lt;p&gt;例如：&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;pkt = packet.Packet(msg.data)    #形成packet对象
pkt_eth = pkt.get_protocols(ethernet.ethernet)[0]    #得到与参数协议匹配的协议字段对象
if not pkt_ethernet:
return   # non Ethernet    
eth_dst=pkt_eth.dst
eth_src=pkt_eth.src
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;解释：get_protocols(protocol)中参数是要解析的协议（如ethernet.py中ethernet类匹配出帧头，位于\ryu\lib\packet\中），返回与协议匹配的协议字段对象的列表，因此需要用[0]得到第一个列表中第一个对象。get_protocol(protocol)可以直接返回匹配的第一个对象。&lt;/p&gt;
&lt;p&gt;解析字段pkt_eth属性值选取(如目的地址pkt_eth.dst)参考协议定义（ethernet类的属性）&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;封包生成&lt;/p&gt;
&lt;p&gt;生成空Packet对象-&amp;gt;生成协议对象（ethernet,ipv4等）-&amp;gt; add_protocol()把协议对象依次加入Packet空对象中-&amp;gt;Packet对象调用serialize()方法把协议对象序列化成Packet对象的属性data。data封包生成，可以用于packout类的data参数。&lt;/p&gt;
&lt;p&gt;例如：&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;pkt = packet.Packet()   #生成空的Packet ()对象
pkt.add_protocol(ethernet.ethernet(ethertype =...,dst=...,src =...))
pkt.add_protocol(ipv4.ipv4(dst=...,src=...,proto =...))
pkt.add_protocol(icmp.icmp(type_ =...,code =...,csum =...,data =...))
pkt.serialize ()   #将添加协议对象序列化为Packet ()对象的data属性
bin_packet = pkt.data  #提取封包好data属性
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;另一种写法（用API）：&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;e = ethernet.ethernet (...)
i = ipv4.ipv4 (...)
u = udp.udp (...)
pkt = e/i/u
&lt;/pre&gt;&lt;/div&gt;


&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;通过以上分析，基本可以完成了openflow包的属性解析与封装，这对写控制器app时候，可以促进我们理解如何处理openflow消息。&lt;/p&gt;</summary><category term="RYU souce code"></category></entry><entry><title>Ryu源码分析</title><link href="http://ysywh.github.io/pages/2015/07/10/Ryu%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90.html" rel="alternate"></link><updated>2015-08-30T17:30:00+08:00</updated><author><name>ysy</name></author><id>tag:ysywh.github.io,2015-07-10:pages/2015/07/10/Ryu源码分析.html</id><summary type="html">&lt;p&gt;Ryu是一个Python写的轻量级控制器，具有易于学习实现的优点。使用ryu进行app开发，有必要对以下几个概念做基本了解：&lt;/p&gt;
&lt;h4&gt;1 Event类&lt;/h4&gt;
&lt;p&gt;位置：ryu\lib\alert.py中Event类&lt;/p&gt;
&lt;p&gt;有一些属性，比如datapath，msg等。其中，datapath是接收到openflow消息的交换机实体，msg就是openflow消息实体。&lt;/p&gt;
&lt;h3&gt;2 讯息实体msg的类：MsgBase()类（讯息实体的基本类）&lt;/h3&gt;
&lt;p&gt;位置：ryu\ofproto\ofproto_parser中 MsgBase()类，&lt;/p&gt;
&lt;p&gt;属性：datapath，version，msg_type，msg_len，xid，buf 等，属于所有讯息实体msg的基本属性。此类会被其他产生讯息的类继承。&lt;/p&gt;
&lt;p&gt;比如packetin消息，会继承这些基本属性外，还具体自己的属性，如match,data等。&lt;/p&gt;
&lt;h3&gt;3 datapath&lt;/h3&gt;
&lt;p&gt;类位置：ryu\controller\controller.py中&lt;/p&gt;
&lt;p&gt;Datapath(ofproto_protocol.ProtocolDesc) 可以看出datapath继承了ProtocolDesc类（见下条，继承的类中得到datapath的两个属性）&lt;/p&gt;
&lt;p&gt;属性:id，ports，ofproto,ofproto_parser（继承来的）等&lt;/p&gt;
&lt;p&gt;方法：send_msg，还有send_packet_out，send_flow_mod等，其实内部最后都调用了
send_msg()&lt;/p&gt;
&lt;h3&gt;4  datapath的两个属性ofproto,ofproto_parser&lt;/h3&gt;
&lt;p&gt;dp = ofproto_protocol.ProtocolDesc(version=ofproto_v1_3.OFP_VERSION)  &lt;/p&gt;
&lt;p&gt;位置：\ryu\ofproto\ofproto_protocol.py中定义了类ProtocolDesc&lt;/p&gt;
&lt;p&gt;解释：通过ProtocolDesc类构造函数（&lt;strong&gt;init&lt;/strong&gt;）获得属于datapath的两个属性ofproto,ofproto_parser(即相应版本的协议模块名和协议解析模块名字，比如模块ofproto_v1_3, ofproto_v1_3_parser)，则这个模块成为data path的属性，通过dp.ofproto_parser可以调用相应版本的类、函数等。&lt;/p&gt;
&lt;h3&gt;5 传统数据包解析&lt;/h3&gt;
&lt;p&gt;传统包头协议位置：\ryu\lib\packet 中有ethernet ipv4等模块，通过Packet编解码传统包。
用法：pkt = packet.Packet(msg.data)  通过Packet类解析出传统数据包&lt;/p&gt;
&lt;p&gt;eth=pkt.get_protocols(ethernet.ethernet)[0] 通过ethernet.py中ethernet类匹配出帧头
    dst = eth.dst
    src = eth.src&lt;/p&gt;
&lt;p&gt;帧头属性（如dst , src查看ethernet类即可）&lt;/p&gt;
&lt;h3&gt;6 ofproto组件（文件夹）：&lt;/h3&gt;
&lt;p&gt;位置：\ryu\ofproto   &lt;/p&gt;
&lt;p&gt;包含openflow协议相关定义与处理,这里对其中几个重要模块（文件）进行分析。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;6.1  ofproto_protocol.py模块（协议指定）&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;作用：协议指定&lt;/p&gt;
&lt;p&gt;过程：通过里面的ProtocolDesc()类来初始化data path的两个属性(如初始化出ofproto_v1_3, ofproto_v1_3_parser)，以此完成OF协议版本的指定&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;6.2  ofproto_parser.py模块（产生讯息后，对讯息解析，产生msg讯息实体）&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;作用：针对交换机发往控制器openflow讯息的解析。这个是为了之后得到讯息实体msg，msg即控制器和交换机之间openflow包(除去包头)的负载部分，即openflow内容。&lt;/p&gt;
&lt;p&gt;过程：针对交换机发往控制器讯息的解析，框架自动调用header()和msg()方法，依次解析讯息（根据OF协议版本）。&lt;/p&gt;
&lt;p&gt;除此，里面有MsgBase()类，将被协议解析模块中的类继承，以此获得讯息实体(msg)。&lt;/p&gt;
&lt;p&gt;这里说明openflow消息的解析与构造流程：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;交换机发往控制器：&lt;/p&gt;
&lt;p&gt;协议指定 -&amp;gt; openflow讯息产生,讯息自动解析(框架自动完成) -&amp;gt; 产生讯息实体msg（ev.msg） -&amp;gt;讯息实体msg解析(通过msg属性，如msg.match得到匹配部分)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;控制器发往交换机：
    协议指定 -&amp;gt;讯息实体msg的构造(形成match和actions等)-&amp;gt;产生讯息openflow消息(如OFPPacketOut) –&amp;gt;发送讯息(send_msg())&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;注意：本节注意讯息（openflow消息）和讯息实体（msg，继承自MsgBase）概念的区分&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;6.3  of协议解析模块（如：ofproto_v1_3_parser.py）&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;文件中定义的类比较多，分类别讨论：&lt;/p&gt;
&lt;p&gt;这里，主要讨论继承类MsgBase()的Openflow讯息类,这些类产生交互的讯息实体msg&lt;/p&gt;
&lt;p&gt;作用：根据控制器和交换机交换交互的包，用模块中相应类产生出讯息实体msg（of负载部分）。这种类分两种：第二种情况，msg实体在类中得到相应属性，并继承MsgBase()一些属性,而第一种情况需要主动调用类，写入属性值。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;第一种，交换机发往控制器消息：&lt;/p&gt;
&lt;p&gt;举例：控制器收到的PacketIn包的类定义：&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;@_register_parser
@_set_msg_type(ofproto.OFPT_PACKET_IN)
class OFPPacketIn(MsgBase):
def __init__(self, data path, buffer_id=None, total_len=None, reason=None,table_id=None, cookie=None, match=None, data=None):
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;分析：对于packin包，通过这个类产生相应讯息实体msg，msg有两种属性：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;特定Msg属性：对于OFPPacketIn的msg，里面有datapath,    buffer_id,total_len,reason,tab
    le_id,cookie,match,data属性，其中data即传统数据包的以太帧，match是OFPMatch的对象实例，实例中有一个40元组的字典型属性（暂且这么理解），键比如in_port，通过match[in_port]访问匹配域的入端口，对象match竟然可以这么访问，OFPMatch中定义的吧）&lt;/li&gt;
&lt;li&gt;基本属性：因为都继承类MsgBase(),则也继承了类MsgBase()中属性，作为msg基本属性：
datapath，version，msg_type，msg_len，xid，buf   &lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;原理：&lt;/p&gt;
&lt;p&gt;类OFPPacketIn是交换机发给控制器讯息的类，对于交换机发给控制器的讯息，会触发此类,但由于此类用修饰符@_register_parse注册，则此类生成实体前，会先在修饰符函数_register_parse中调用此类中的parser函数进行对讯息进行解析，完成此类的msg实例化，产生讯息实体msg。&lt;/p&gt;
&lt;p&gt;使用方法：如处理packin消息：&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;@set_ev_cls(ofp_event.EventOFPPacketIn, MAIN_DISPATCHER)
def packet_in_handler(self, ev):
msg = ev.msg   得到msg实体
…
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;解释：以上原理中所说的（Of讯息产生，并解析出msg讯息实体）相当于产生一个对应事件ev，应自定义事件管理函数（如packet_in_handler）对事件进行处理。&lt;/p&gt;
&lt;p&gt;事件管理函数(Event hander)是以接收的事件ev为形参，使用@set_ev_cls（`ryu.controller.handler.set_ev_cls）修饰的函数。修饰符函数@set_ev_cls中第一个参数接受的讯息（Event+讯息类名），第二个参数有四种情况，描述交换机状态。（ryu.controller.handler中定义）。&lt;/p&gt;
&lt;p&gt;其中第一个参数定义见ryu.controller.ofp_event.py，其中函数_create_ofp_msg_ev_class(msg_cls) 调用函数_ofp_msg_name_to_ev_name(msg_cls)产生事件名字作为set_ev_cls参数。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;def _ofp_msg_name_to_ev_name(msg_name):
return &amp;#39;Event&amp;#39; + msg_name
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;其中ev.msg是用来储存对应事件的OpenFlow 讯息实体。上例中是ryu.ofproto.ofproto_v1_3_parser. OFPPacketIn。&lt;/p&gt;
&lt;p&gt;msg.datapath是存储OpenFlow交换器ryu.controller.controller.Datapath类所对应的实体。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;第二种，控制器发往交换机消息&lt;/p&gt;
&lt;p&gt;作用：这些类无@_register_parse的类（如OFPPacketOut），这些类用来控制器下发讯息产生讯息实体msg，方法是手动书写调用此类，参数是类应有的属性值如datapath，和第一种过程恰好相反。产生讯息实体msg后通过msg的方法msg_send（msg）产生讯息发出去。&lt;/p&gt;
&lt;p&gt;用法（根据类定义）：如类OFPPacketOut用法：&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;def send_packet_out(self, datapath, buffer_id, in_port):  #自定义
    ofp = datapath.ofproto
    ofp_parser = datapath.ofproto_parser
    actions = [ofp_parser.OFPActionOutput(ofp.OFPP_FLOOD, 0)]
    req=ofp_parser.OFPPacketOut(datapath, buffer_id,in_port, actions) #调用类形成讯息实体msg
    datapath.send_msg(req)  #使用datapath方法把msg发出去
&lt;/pre&gt;&lt;/div&gt;


&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;对于第一种情况（交换机发往控制器）的几种类：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;OFPPacketIn(MsgBase):&lt;/li&gt;
&lt;li&gt;OFPFlowRemoved(MsgBase):当交换机流表项被删除或是过期，向控制器发此消息&lt;/li&gt;
&lt;li&gt;OFPPortStatus(MsgBase)：当交换机端口状态改变，向控制器发此消息&lt;/li&gt;
&lt;li&gt;OFPErrorMsg(MsgBase): 交换机发往控制器关于发生错误的消息&lt;/li&gt;
&lt;li&gt;OFPGetConfigReply(MsgBase)：对于OFPGetConfigRequest的回答消息&lt;/li&gt;
&lt;li&gt;对于b情况（控制器下发给交换机）有类：&lt;/li&gt;
&lt;li&gt;OFPFlowMod(MsgBase)：控制器向交换机发改变流表项的消息&lt;/li&gt;
&lt;li&gt;OFPPacketOut(MsgBase)：控制器向交换机发的消息&lt;/li&gt;
&lt;li&gt;OFPGetConfigRequest(MsgBase)：控制器发往交换机的交换机配置信息请求信息&lt;/li&gt;
&lt;li&gt;OFPSetConfig(MsgBase): 控制器下发的交换机配置设置的要求消息，无需reply&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Ryu框架自动调用完成的类：OFPHello，OFPEchoReply，OFPEchoRequest，OFPSwitchFeatures（回答控制器发往交换机OFPFeaturesRequest，前面四个是交换机发往控制器的）&lt;/p&gt;
&lt;p&gt;注意：此类还有很多，只要是继承MsgBase的类，都属于产生控制器交换机交互讯息的类。&lt;/p&gt;</summary><category term="RYU souce code"></category></entry><entry><title>OVS源码分析——datapath消息处理</title><link href="http://ysywh.github.io/pages/2015/05/22/OVS%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90%E2%80%94%E2%80%94datapath%E6%B6%88%E6%81%AF%E5%A4%84%E7%90%86.html" rel="alternate"></link><updated>2015-08-30T17:30:00+08:00</updated><author><name>ysy</name></author><id>tag:ysywh.github.io,2015-05-22:pages/2015/05/22/OVS源码分析——datapath消息处理.html</id><summary type="html">&lt;p&gt;&lt;/br&gt;&lt;/p&gt;
&lt;h3&gt;前言&lt;/h3&gt;
&lt;p&gt;当packet进入datapath无法匹配则通过netlink上交给用户态，处理完成后用户态会通过generic netlink与内核态进行交互，把流表项flow和packet发给内核态进行处理。下面分析内核态如何通过generic netlink接收flow与packet，并且进行相应处理。&lt;/p&gt;
&lt;p&gt;generic netlink：ovs的内核和用户态通过generic netlink(genl，泛型netlink，比netlink支持更多协议，功能更多)交互。genl任然是cs模式，内核为服务端，用户态为客户端（内核中还有一个genl控制器，属于内核态的客户端，用来分配通信通道）。类似socket，genl建立一系列通信通道，genl family对应这些通道，从而交互。&lt;/p&gt;
&lt;p&gt;通信前，作为服务端的内核态要定义family（指定信道类型）和operation（处理函数），并且进行注册，则客户端可以通过注册的信息与之交互。直接以ovs内核态为例分析：&lt;/p&gt;
&lt;h4&gt;&lt;strong&gt;一 内核态对flow的接收与处理&lt;/strong&gt;&lt;/h4&gt;
&lt;p&gt;1 定义注册genl数据结构（datapath\datapth.c）：&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;static struct genl_family dp_flow_genl_family = {
    .id = GENL_ID_GENERATE,
    .hdrsize = sizeof(struct ovs_header),
    .name = OVS_FLOW_FAMILY,
    .version = OVS_FLOW_VERSION,
    .maxattr = OVS_FLOW_ATTR_MAX,
     SET_NETNSOK
};
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;这个是genl family结构体定义，名字dp_flow_genl_family，用来处理flow的family。参数都是genl family结构体的部分属性，对其进行初始化。id和name即为family id和name。hdrsize定义ovs协议头长度，genl使用netlink标准的attr来传输数据。注意，客户端通过name就可以发送数据，让指定的family接收。如一种8的dpif_linux_flow_to_ofpbuf函数中，就会调用函数nl_msg_put_genlmsghdr，传入参数ovs_flow_family，从而制定了内核接收到的通道family。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;static struct genl_ops dp_flow_genl_ops[] = {
    { .cmd = OVS_FLOW_CMD_NEW,
      .flags = GENL_ADMIN_PERM, /* Requires CAP_NET_ADMIN privilege. */
      .policy = flow_policy,
      .doit = ovs_flow_cmd_new_or_set
    },…..
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;这个是genl operation的定义，名为dp_flow_genl_ops，主要用来定义对于flow的处理函数的定义。由于定义了数组，值罗列第一个genl ops。cmd是命令名字，可以看出处理函数的功能（这里主要是加入、更新或删除流表项flow），flag是属性设置，policy是接收到数据后，但没有触发事件处理函数之前所执行的过滤准则，比如对帧中数据attr的校验。doit是回调函数，generic netlink接收到数据触发，运行在进程上下文中，是对来自用户态数据进行执行操作的重点。doit函数接收skb和genl_info两个数据，skb即为触发此回调函数的socket buffer，genl_info结构体包含发送序号、发送客户端pid，头部指针和attrs(即经过genl_ops-&amp;gt;policy过滤后的结果，包含传递的数据。)。当然有的还有回调函数dumpit, dumpit与doit的区别是:dumpit的第一个参数skb不会携带从客户端发来的数据.
定义实例化完family和ops后，需要通过genl_register_family和genl_register_ops进行注册。这样则可以进行通信。&lt;/p&gt;
&lt;p&gt;2 调用处理函数进行流表项处理操作。
定义注册完famliy和ops，即可与用户态进行交互（用户态通过指定famliy指定通信通道，从而内核态接收数据会触发相应的ops回调函数进行处理）。下来分析来自用户态下发flow的流表项处理。&lt;/p&gt;
&lt;p&gt;&lt;code&gt;static int ovs_flow_cmd_new_or_set(struct sk_buff *skb, struct genl_info *info)&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;这个是内核态对于flow的增加或是更新（action更新）的处理函数。判断不存在相同流表项，则扩展table、分配流表项空间然后把流表项flow放入bucket中。存在的话则更新action即可。&lt;/p&gt;
&lt;p&gt;具体分析一下，首先需要把用户态传递过来的数据提取出来**a = info-&amp;gt;attrs，前面分析可知，传入参数info这个结构体数据成员attrs，attrs既是policy过滤后的数据。之后，调用函数ovs_flow_from_nlattrs，从a中提取key，调用validate_and_copy_actions把a中action放入变量acts（此处判断若为OVS_FLOW_CMD_NEW类型，即对于增加新流表项却没有action报错）。此后利用get_dp函数得到插入流表项的datapath（info-&amp;gt;userhdr-&amp;gt;dp_ifindex-&amp;gt;net_device -&amp;gt; vport -&amp;gt; datapath），继而找到相应流表table。&lt;/p&gt;
&lt;p&gt;为了确认是否已存在相同流表项，先查找流表项flow = ovs_flow_tbl_lookup(table, &amp;amp;key, key_len);主要通过在table中匹配key查找。先分析不存在flow为false，则进入加入新流表项if语句中，先排除不是更新流表项操作（OVS_FLOW_CMD_SET），然后调用函数ovs_flow_tbl_expand扩展流表table空间，方便之后添加新流表项，然后分配流表项空间ovs_flow_alloc。下来即重要函数ovs_flow_tbl_insert，其向bucket中插入流表项flow，调用ovs_flow_cmd_build_info构造回复用户层的通知消息。对于存在流表项即只更新action部分，不再赘述。&lt;/p&gt;
&lt;p&gt;对于ops中其他回调函数，如ovs_flow_cmd_del（删除流表项）原理一样，主要通过判断flow是否存在，存在则调用ovs_flow_tbl_remove (table, flow)函数删除流表项。&lt;/p&gt;
&lt;p&gt;3 插入流表项&lt;/p&gt;
&lt;p&gt;&lt;code&gt;void ovs_flow_tbl_insert(struct flow_table *table, struct sw_flow *flow,
             struct sw_flow_key *key, int key_len)&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;关系：在ovs_flow_cmd_new_or_set中调用&lt;/p&gt;
&lt;p&gt;功能：插入流表项。&lt;/p&gt;
&lt;p&gt;数据结构&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;sw_flow：流表项flow的数据结构&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;主要数据成员&lt;ul&gt;
&lt;li&gt;sw_flow_key类型的key:key可以称为查找key，就是匹配流表项的匹配域字段组合（port、优先级、l2-l4），&lt;/li&gt;
&lt;li&gt;sw_flow_actions类型的sf_acts:sf_acts用来存贮action的类型。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;flow_table：流表结构体&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;主要成员&lt;ul&gt;
&lt;li&gt;flex_array类型的bucket&lt;/li&gt;
&lt;li&gt;锁rcu_head的rcu。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;之前2中分析了内核态接收到消息，提取出key和action等，找出相应datapath和table，则向table中插入流表项，现在主要分析如何向table插入流表项flow。&lt;/p&gt;
&lt;p&gt;在ovs_flow_tbl_insert函数前，先调用函数rcu_assign_pointer(flow-&amp;gt;sf_acts, acts)把action插入到空白流表项flow的sf_acts（flow的action）字段中。则调用流表项插入函数ovs_flow_tbl_insert，传入table、流表项flow（目前只有action字段初始化了）、匹配域key等参数。函数中，调用函数ovs_flow_hash把key 进行hash后放入flow-&amp;gt;hash再通过memcpy函数把key复制到flow的key字段，即此时flow流表项就构成了，有了匹配域key和action两个重要部分。此时调用__flow_tbl_insert函数把flow插入table相应位置即可。函数中，先通过find_bucket函数，利用flow-&amp;gt;hash在table中找到应插入的哈希桶位置头部head，然后通过函数hlist_add_head_rcu把头部位置head写入flow-&amp;gt;hash_node参数即完成flow插入。可以看到flow中key的hash可以定位插入table的哈希桶位置。&lt;/p&gt;
&lt;h4&gt;&lt;strong&gt;二 内核态接收处理packet&lt;/strong&gt;&lt;/h4&gt;
&lt;p&gt;内核态接受处理packet和接受处理flow原理类似，也是先定义注册generic netlink的family（name为OVS_PACKET_FAMILY）和ops，见用户层8）中调用函数dpif_linux_encode_execute发送packet包时，会指明OVS_PACKET_FAMILY通道，则会触发此通道的ops回调函数进行packet接收处理。下来详细对此回调函数ovs_packet_cmd_execute进行分析。&lt;/p&gt;
&lt;p&gt;1 内核态调用回调函数接收处理packet &lt;/p&gt;
&lt;p&gt;&lt;code&gt;static int ovs_packet_cmd_execute(struct sk_buff *skb, struct genl_info *info)&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;位置：datapath/datapath.c&lt;/p&gt;
&lt;p&gt;调用memcpy函数把netlink接收到数据a的packet部分(应该是传统包内容)提取到packet参数中，a中还有action等字段。接下来会构造一个sw_flow结构体的flow来发送packet：&lt;/p&gt;
&lt;p&gt;调用ovs_flow_extract函数，从packet的以太网帧头中提取key部分构造流表项flow的匹配域key，此
外，还会调用函数ovs_flow_metadata_from_nlattrs直接从a的key字段解析补充flow的key，这些是a的packet中提取不到的一些字段（如tunnel id）。调用validate_and_copy_actions函数把actions提取到变量acts中，从而进一步利用函数rcu_assign_pointer把动作acts放入flow的action字段sf_acts中。此时匹配此packet的flow流表项构成。调用OVS_CB(packet)-&amp;gt;flow = flow;把flow放入packet的OVS_CB中（不理解？在后面处理packet时候可以直接关联找到此packet的action）。获取dp后调用函数ovs_execute_actions执行packet的动作。&lt;/p&gt;
&lt;p&gt;2 执行packet关联的所有action&lt;/p&gt;
&lt;p&gt;&lt;code&gt;int ovs_execute_actions(struct datapath *dp, struct sk_buff *skb)&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;位置：datapath\actions.c&lt;/p&gt;
&lt;p&gt;最开始就直接通过acts = rcu_dereference(OVS_CB(skb)-&amp;gt;flow-&amp;gt;sf_acts)提取出packet（即skb）的所有actio n。然后函数do_execute_actions（datapath\actions）传入dp、skb和action对packet执行一系列actions。在函数do_execute_actions中，通过逐一循环判断action类型，从而执行相应的操作。&lt;/p&gt;
&lt;p&gt;这里主要讨论包处理类型为从端口输出，则会从packet中提取output端口号到参数pre_port，从而每次循环结束后调用do_output函数发送packet包，发送时候会对包进行克隆。&lt;/p&gt;
&lt;p&gt;在do_output函数中，通过调用函数ovs_vport_rcu，利用参数dp和out_port得到发送端口实体vport（datapath端口结构体），从而调用ovs_vport_send(vport, skb)把packet发送到指定vport。&lt;/p&gt;
&lt;p&gt;当然除了输出端口操作，还有push_vlan、pop_vlan、output_userspace（发送包到用户空间）等操作。对于发送用户空间会调用output_userspace函数，函数中调用ovs_dp_upcal（datapath\datapath.c）发送upcall消息，不再赘述。&lt;/p&gt;
&lt;p&gt;3 调用底层设备进行packet发送&lt;/p&gt;
&lt;p&gt;结构体：vport（datapath\vport.h）端口号、所属dp外有一个重要成员vport_ops，定义了端口所能执行的所有操作行为。&lt;/p&gt;
&lt;p&gt;vport_ops：定义datapath的端口vport的所有行为函数。如send，用来从设备发送packet。
首先初始化结构体vport_ops为ovs_netdev_vport_ops（datapath\vport-netdev.c）。定义了vport的所有操作函数，比如.send=netdev_send。
static int netdev_send(struct vport &lt;em&gt;vport, struct sk_buff &lt;/em&gt;skb)&lt;/p&gt;
&lt;p&gt;关系：在函数ovs_vport_send中，通过vport-&amp;gt;ops-&amp;gt;send(vport, skb)动态调用。&lt;/p&gt;
&lt;p&gt;函数中，通过skb-&amp;gt;dev = netdev_vport-&amp;gt;dev;得到设备，然后调用dev_queue_xmit(skb)函数把packet放入队列中发送。&lt;/p&gt;</summary><category term="OVS souce code"></category></entry><entry><title>OVS源码分析——用户层处理Upcall消息(2)</title><link href="http://ysywh.github.io/pages/2015/05/15/OVS%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90%E2%80%94%E2%80%94%E7%94%A8%E6%88%B7%E5%B1%82%E5%A4%84%E7%90%86Upcall%E6%B6%88%E6%81%AF(2).html" rel="alternate"></link><updated>2015-08-30T17:30:00+08:00</updated><author><name>ysy</name></author><id>tag:ysywh.github.io,2015-05-15:pages/2015/05/15/OVS源码分析——用户层处理Upcall消息(2).html</id><summary type="html">&lt;p&gt;&lt;/br&gt;&lt;/p&gt;
&lt;h3&gt;前言&lt;/h3&gt;
&lt;p&gt;当OVS内核接从端口到packet，会进行包解析提取出查询key，然后用key进行datapath中的流表匹配处理，对于以下两种情况1）没有匹配到流表项 2）匹配到的流表项动作为发往用户层，都会将packet和
key构造成Upcall消息，通过netlink通道发往用户层，用户态接收upcall消息后会进行处理，比如用户层流表匹配处理等。当然在用户层匹配到流表，会将流表项安装到内核层，达到相同流在内核层快速处理的目的：&lt;/p&gt;
&lt;p&gt;上一篇博客分析了用户层从用户层获取upcall消息，对MISS_UPCALL类型消息进行处理并且存入miss（结构体miss_flow）。这篇文章，继续分析对upcall消息的处理、给内核层下发流表项。&lt;/p&gt;
&lt;hr /&gt;
&lt;p&gt;&lt;/br&gt;
接下来要具体对miss进行处理（构造facet和对miss执行action）。&lt;/p&gt;
&lt;h4&gt;4）&lt;strong&gt;&lt;code&gt;static void handle_flow_miss(struct ofproto_dpif *ofproto, struct flow_miss *miss,struct flow_miss_op *ops, size_t *n_ops)&lt;/code&gt;&lt;/strong&gt;&lt;/h4&gt;
&lt;p&gt;&lt;strong&gt;关系&lt;/strong&gt;：在handle_miss_upcalls中被调用&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;功能&lt;/strong&gt;：判断是存在和flow相同的facet，不存在则根据实体的结构体ofproto_dpif-&amp;gt;rule和flow_miss-&amp;gt;flow创建facet。存在facet后进而对flow_miss进行处理。&lt;/p&gt;
&lt;p&gt;数据结构&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;facet（ofproto\ofproto-dpif.c）： openflow flow的exact-match实体。个人理解是用户空间的流表实体，用来匹配upcall消息或是openflow消息的，然后执行动作，类比匹配传统流的流表。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;主要数据成员&lt;ul&gt;
&lt;li&gt;struct rule_dpif *rule   &lt;/li&gt;
&lt;li&gt;struct list subfacets&lt;/li&gt;
&lt;li&gt;struct flow flow;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;1 从ofproto（结构体ofproto_dpif）的成员facet表中找到与flow完全匹配的项。&lt;/p&gt;
&lt;p&gt;其中，&lt;code&gt;facet = facet_lookup_valid(ofproto, &amp;amp;miss-&amp;gt;flow, hash)&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;实现是通过函数中调用facet = facet_find(ofproto, flow, hash)，facet_find通过存在的ofproto-&amp;gt; facets-&amp;gt;flow与flow相比，看是否完全相等，相等则返回在各方存在的facet。不存在否则返回空指针赋给facet。&lt;/p&gt;
&lt;p&gt;2 if判断facet存在性，如果存在直接调用handle_flow_miss_with_facet进行后续处理，否则（比如第一个upcall到达时候，这个才是常见情况）需要得到rules，进而创建一个facet，再调用handle_flow_miss_with_facet进行后续处理。&lt;/p&gt;
&lt;p&gt;创建facet,需要先得到rule:&lt;/p&gt;
&lt;p&gt;&lt;code&gt;struct rule_dpif *rule = rule_dpif_lookup(ofproto, &amp;amp;miss-&amp;gt;flow)&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;函数中，先通过rule_dpif_lookup__函数查找返回结构体为rule_dpif的规则rule。如果流表（？）中没有查找到，则通过rule_dpif_miss_rule函数创建rule，rule取值类型为结构体ofproto_dpif中的miss_rule或no_packet_in_rule类型。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;进入函数，这里有必要分析一下rule_dpif_miss_rule函数：&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;code&gt;static struct rule_dpif * rule_dpif_miss_rule(struct ofproto_dpif *ofproto, const struct flow *flow)&lt;/code&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;数据结构:rule_dpif，具有重要成员rule。&lt;ul&gt;
&lt;li&gt;rule：是一个包含结构体ofproto（可看做一个openflow交换机）的openflow flow(openflow流，可看做比如flowmod这种openflow流？)。最重要是具有ofpact（action头部）这个结构体。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;主要成员：最主要是 ofpact（action header，标注了action的类型等）。其次有成员hardtime，idle_timeout，table_id，flow_cookie等。&lt;/p&gt;
&lt;p&gt;rule_dpif_miss_rule函数中，通过判断选择返回rule_dpif的类型miss_rule或no_packet_in_rule。miss_rule类型代表发往控制器，no_packet_in_rule代表丢弃。&lt;/p&gt;
&lt;p&gt;这里可以分析一下port-&amp;gt;up.pp.config，化为结构体为ofproto_dpif-&amp;gt;ofport-&amp;gt;ofputil_phy_port-&amp;gt;ofputil_port_config。结构体ofport是openflow交换机的一个端口实体，调用物理端口抽象fputil_phy_port，有配置、状态、最大速率等参数成员，配置参数结构体ofputil_port_config有成员不允许转发、不允许flood等成员参数。&lt;/p&gt;
&lt;p&gt;3 通过2得到了rule，这里结合miss-&amp;gt;flow创建facet。&lt;/p&gt;
&lt;p&gt;&lt;code&gt;facet = facet_create(rule, &amp;amp;miss-&amp;gt;flow, hash)&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;在这之前，有函数flow_miss_should_make_facet，判断是否在用户空间追踪安装datapath flow（不太理解，往往是不进入if内部）。接下来，则函数facet_create通过rule和flow创建facet，但却还没有subfacet，且相应action在rule中。&lt;/p&gt;
&lt;p&gt;4 至此寻找或是创建了facet，则有了和flow_miss匹配的facet，则对flow_miss具体处理，且其中增加datapath操作给ops，且更新计数n_ops。&lt;/p&gt;
&lt;p&gt;其中，&lt;code&gt;handle_flow_miss_with_facet(miss, facet, now, ops, n_ops)&lt;/code&gt;
具体分析此函数。目前有了和flow_miss匹配的facet，则具体对flow_miss进行处理&lt;/p&gt;
&lt;p&gt;&lt;/br&gt;&lt;/p&gt;
&lt;h4&gt;&lt;strong&gt;5 ）&lt;code&gt;static void handle_flow_miss_with_facet(struct flow_miss *miss, struct facet *facet,long long int now,struct flow_miss_op *ops, size_t *n_ops)&lt;/code&gt;&lt;/strong&gt;&lt;/h4&gt;
&lt;p&gt;&lt;strong&gt;关系&lt;/strong&gt;：在handle_flow_miss中最后被调用&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;功能&lt;/strong&gt;：给facet创建subfacet，并且填充action动作，继而按照动作处理flow_miss。&lt;/p&gt;
&lt;p&gt;结构体：subfacet ：包含facet的action的dpif flow（A dpif flow and actions associated with a facet.）&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;主要成员
    -struct nlattr *actions; （Datapath actions）&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;1 创建facet的subfacet&lt;/p&gt;
&lt;p&gt;&lt;code&gt;subfacet = subfacet_create(facet,miss-&amp;gt;key_fitness, miss-&amp;gt;key, miss-&amp;gt;key_len,
                               miss-&amp;gt;initial_tci, now)&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;函数中先判断facet的成员subfacet是否为空，不为空则直接返回subfacet，否则创建subfacet，并向subfacet中各个成员字段赋空值。则对于新建subfacet的action等成员都为空。之后，要依赖subfacet_make_actions函数创建action。&lt;/p&gt;
&lt;p&gt;2 填充subfacet的action字段，作为datapath action。&lt;/p&gt;
&lt;p&gt;&lt;code&gt;subfacet_make_actions(subfacet, packet, &amp;amp;odp_actions)&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;通过rule的action来构造subfacet的datapath action，并把action存入结构体为ofbuf缓存变量odp_actions中。&lt;/p&gt;
&lt;p&gt;函数中，具体先通过函数action_xlate_ctx_init，结合ofproto、flow、rule等初始化出action translate context这个上下文变量ctx。然后调用函数xlate_actions，结合参数ctx、rule等翻译出datapath action并放入odp_actions中，最后利用上下文ctx填充facet一些字段，利用存有action的odp_actions填充subfacet的actions字段。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;这里需要着重分析xlate_actions函数，因为其得到action并具体执行了flow_miss的action
（比如发往控制器）。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;/br&gt;&lt;/p&gt;
&lt;h4&gt;6）&lt;strong&gt;&lt;code&gt;static void xlate_actions(struct action_xlate_ctx *ctx, const struct ofpact *ofpacts, size_t ofpacts_len,struct ofpbuf *odp_actions)&lt;/code&gt;&lt;/strong&gt;&lt;/h4&gt;
&lt;p&gt;&lt;strong&gt;关系&lt;/strong&gt;：在subfacet_make_actions中被最后调用。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;功能&lt;/strong&gt;：利用上下文ctx把datapath action翻译到 odp_actions缓存里，action长度为rule中的ofpacts（action头部）标注的长度。然后执行动作。&lt;/p&gt;
&lt;p&gt;1 首先一个if switch语句进行packet分片处理。。。（？）&lt;/p&gt;
&lt;p&gt;2 &lt;code&gt;special = process_special(ctx-&amp;gt;ofproto, &amp;amp;ctx-&amp;gt;flow, ctx-&amp;gt;packet)&lt;/code&gt;
 判断packet类型是否为SLOW_CFM（连通性故障管理协议）、SLOW_LACP（链路聚合协议）、SLOW_STP，如果是则直接在判断语语句中进行相应处理(slow path)，然后返回相应类型，否则返回0(fast path)，等待后续处理。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;解释slow path:这里需要注意，用户层对于上交包的处理，分为slow通道和非slow通道（ctx的slow参数，0则为fast path，非0则为slow path）。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;变量slow为enum slow_path_reason（lib\odp-util.h），通过看enum枚举可以看到slow path原因：SLOW_CFM、SLOW_LACP、 SLOW_STP、SLOW_IN_BAND（inband模式）等六种packet情况都走slow path。&lt;/p&gt;
&lt;p&gt;3 紧接根据special判断是否为CFM，LACP，STP包，若是则ctx-&amp;gt;slow与相应类似包标志位做或运算，初始化slow path原因，若不是则利用函数do_xlate_actions（根据rule-&amp;gt;up.ofpacts）执行具体的action。这个极其重要，具体分析见后。&lt;/p&gt;
&lt;hr /&gt;
&lt;p&gt;以上对于upcal上来的类型为flow_miss的packet，寻找匹配的facet或是因为第一个packet无法匹配而构造facet（利用rule、flow构造facet、subfacet，利用rule中action构造subfacet的action字段），接下来则重点根据rule-&amp;gt;up.ofpacts类型来执行动作处理上交的包。&lt;/p&gt;
&lt;p&gt;&lt;/br&gt;&lt;/p&gt;
&lt;h4&gt;7）&lt;strong&gt;&lt;code&gt;static void do_xlate_actions(const struct ofpact *ofpacts, size_t ofpacts_len,struct action_xlate_ctx *ctx)&lt;/code&gt;&lt;/strong&gt;&lt;/h4&gt;
&lt;p&gt;&lt;strong&gt;关系&lt;/strong&gt;：在xlate_actions中被调用&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;功能&lt;/strong&gt;：对于上交到用户层的包，根据rule-&amp;gt;up.ofpacts的action类型来执行相应的动作。&lt;/p&gt;
&lt;p&gt;结构体：ofpact：action的头部，属于rule的一个数据成员。（lib\ofp-actions.h）&lt;/p&gt;
&lt;p&gt;数据成员：action类型type、ofputil_action_code和len。其中type是ovs的action类型（如
output类型、Header changes等action类型）。&lt;/p&gt;
&lt;p&gt;1 首先利用函数get_ofp_port得到端口的实现实例port（ofport_dpif类型，ofport_dpif中有ofport类型成员，是一个openflow端口）。紧接着if判断，如果forwarding和learning都是disable，则对包执行drop动作。否则继续执行后面。&lt;/p&gt;
&lt;p&gt;2 进入循环，用switch (a-&amp;gt;type)判断动作类型并执行相应动作。这里特别分析action type为output时的动作执行（如对于第一个包交往控制器则要执行这个动作）。&lt;/p&gt;
&lt;p&gt;&lt;code&gt;xlate_output_action(ctx, ofpact_get_OUTPUT(a)-&amp;gt;port,ofpact_get_OUTPUT(a)-&amp;gt;max_len, true)&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;先用ofpact_get_OUTPUT(a)-&amp;gt;port得到具体端口类型（如controller），然后调用xlate_output_action函数执行动作。&lt;/p&gt;
&lt;p&gt;进入xlate_output_action函数可以看到switch语句，output类型端口有in_port、table/normal、controller、flood等。对于发往controller，执行execute_controller_action函数，跟踪内部，会发现调用packin类似函数发往controller。&lt;/p&gt;
&lt;h2&gt;&lt;/br&gt;&lt;/h2&gt;
&lt;p&gt;以上对于flow_miss，完成了创建rule、facet、subfacet、action即动作的执行，但最终需要用socket完成数据传输，完成真正的执行动作。因此，需要回顾3），handle_flow_miss函数处理完后，需要根据datapath执行类型进行执行，调用函数dpif_operate。在此做简单分析。&lt;/p&gt;
&lt;p&gt;&lt;/br&gt;&lt;/p&gt;
&lt;h4&gt;&lt;strong&gt;8）&lt;code&gt;void dpif_operate(struct dpif *dpif, struct dpif_op **ops, size_t n_ops)&lt;/code&gt;&lt;/strong&gt;&lt;/h4&gt;
&lt;p&gt;&lt;strong&gt;关系&lt;/strong&gt;：在函数handle_miss_upcalls最后被调用&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;功能&lt;/strong&gt;：根据之前得到的datapath操作类型批量处理，通过nla发到内核，从而让内核可以真正执行操作，发送完并接收回应来更新统计信息。&lt;/p&gt;
&lt;p&gt;数据结构：dpif_op：datapath操作类型。数据成员有三种操作类型的数据结构成员：dpif_flow_put、dpif_flow_del、dpif_execute。&lt;/p&gt;
&lt;p&gt;这三种操作类型的数据结构中基本都有重要的成员：dpif_flow_put（成员key和actions）、dpif_flow_del（成员key），dpif_execute（成员key、actions和packet）。成员key是待要进行put或是delete的flow（流表项），packet（对于dpif_exacute）是要进行执行的packet。&lt;/p&gt;
&lt;p&gt;dpif_operate函数调用dpif-&amp;gt;dpif_class-&amp;gt;operate(dpif, ops, n_ops)进行批量处理，没有处理的之后再以根据操作类型依次处理。dpif-&amp;gt;dpif_class-&amp;gt;operate(dpif, ops, n_ops)具体动态调用了dpif_linux_operate（lib\dpif-linux.c），而在dpif_linux_operate中调用dpif_linux_operate__循环处理，此函数中有具体处理过程。&lt;/p&gt;
&lt;p&gt;&lt;code&gt;static void dpif_linux_operate__(struct dpif *dpif_, struct dpif_op **ops, size_t n_ops)&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;根据处理三种处理类型，循环遍历所有操作ops（带有即将处理key和action）,将key与action放入缓存request中。之后执行socket与内核通信。&lt;/p&gt;
&lt;p&gt;1 先定义变量 *txnsp[MAX_OPS]，其为nl_transaction结构体（具有类型为ofpbuf的request和reply缓存变量），request为发送缓存，reply为接收缓存。Txnsp用来表征内核层与用户层之间的一次通信。真正socket连接通信时候会调用。&lt;/p&gt;
&lt;p&gt;2 根据操作类型分别处理，主要是想把action和key(excute类型还有packet)放入aux的request缓存中，方便以后传到txnps中再进行发送。根据是处理flow（增加或删除流表项）还是packet（packet关联原始经过交换机的传统包内容）操作分为三种：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;第一种：put流表项flow。&lt;/p&gt;
&lt;p&gt;&lt;code&gt;dpif_linux_init_flow_put(dpif_, put, &amp;amp;flow)&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;这里主要是把变量put内容（主要是action和key，key内容即为流表项flow）传到flow变量中（其中通过前面put = &amp;amp;op-&amp;gt;u.flow_put把ops中key和action传到put中）。&lt;/p&gt;
&lt;p&gt;dpif_linux_flow_to_ofpbuf(&amp;amp;flow, &amp;amp;aux-&amp;gt;request)&lt;/p&gt;
&lt;p&gt;函数中又进一步把ovsheader和flow中的key与action填充到发送缓存request中（通过调用ofpbuf_put_uninit和nl_msg_put_unspec），从而得到与内核层交互发送信息。&lt;/p&gt;
&lt;p&gt;函数中，还制定了内核中数据接收通道famliy名字，从而指定此类消息被内核什么处理函数接收处理。利用函数nl_msg_put_genlmsghdr(buf, 0, ovs_flow_family…即内核接收family为ovs_flow_family（内核中数据交互前已经注册好，用户态直接连接交互即可）。
- 第二种：delete流表项flow，这与第一种相同，不过缺少像put操作存入缓存的action。
- 第三种：packet的处理&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;code&gt;dpif_linux_encode_execute(dpif-&amp;gt;dp_ifindex, execute,&amp;amp;aux-&amp;gt;request)&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;这个函数和前两种类似，不过直接通过execute = &amp;amp;op-&amp;gt;u.execute;把内容传到excute变量，再通过函数传到aux-&amp;gt;request缓存（主要传入有key、packet和action，特别packet）。&lt;/p&gt;
&lt;p&gt;3 &lt;code&gt;nl_sock_transact_multiple(genl_sock, txnsp, n_ops)&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;与内核层建立socket的netlink连接，发送txnsp的request中信息（通过2中的aux传到txnsp中）并且接收来自内核的消息到reply中。参数genl_sock是netlink的socket, txnsp即表征内核层与用户层的一次连接，txnsp含有成员request和reply。&lt;/p&gt;
&lt;p&gt;函数中，具体调用了nl_sock_transact_multiple__函数，此函数中有发送函数sendmsg(sock-&amp;gt;fd, &amp;amp;msg, 0)和接收函数nl_sock_recv__(sock, buf_txn-&amp;gt;reply, false)，分别发送了缓存request
的信息并接收到reply缓存中，此后更新统计。&lt;/p&gt;
&lt;p&gt;至此，用户层信息发送到内核层，内核层真正执行动作。内核层和用户层交互完成。&lt;/p&gt;</summary><category term="OVS souce code"></category></entry><entry><title>OVS源码分析——用户层处理Upcall消息(1)</title><link href="http://ysywh.github.io/pages/2015/05/10/OVS%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90%E2%80%94%E2%80%94%E7%94%A8%E6%88%B7%E5%B1%82%E5%A4%84%E7%90%86Upcall%E6%B6%88%E6%81%AF(1).html" rel="alternate"></link><updated>2015-08-30T17:30:00+08:00</updated><author><name>ysy</name></author><id>tag:ysywh.github.io,2015-05-10:pages/2015/05/10/OVS源码分析——用户层处理Upcall消息(1).html</id><summary type="html">&lt;p&gt;&lt;/br&gt;&lt;/p&gt;
&lt;h3&gt;前言&lt;/h3&gt;
&lt;p&gt;当OVS内核接从端口到packet，会进行包解析提取出查询key，然后用key进行datapath中的流表匹配处理，对于以下两种情况1）没有匹配到流表项 2）匹配到的流表项动作为发往用户层，都会将packet和
key构造成Upcall消息，通过netlink通道发往用户层，用户态接收upcall消息后会进行处理，比如用户层流表匹配处理等。当然在用户层匹配到流表，会将流表项安装到内核层，达到相同流在内核层快速处理的目的：&lt;/p&gt;
&lt;p&gt;下俩将按照用户层接收upcall消息与处理的流程进行分析：&lt;/p&gt;
&lt;p&gt;用户层接收upcall消息，通过ofproto实体完成底层消息接收功能的。ofproto是ofproto_dpif类型数据结构，具有数据成员dpif（ofproto\ofproto-dpif.c）。通过ofproto—&amp;gt;dpif与内核交互数据。&lt;/p&gt;
&lt;p&gt;upcall处理流程如图:&lt;/p&gt;
&lt;p&gt;&lt;/br&gt;&lt;/p&gt;
&lt;h4&gt;&lt;strong&gt;1）   upcall消息处理函数handle_upcalls(ofproto\ofproto-dpif.c)&lt;/strong&gt;&lt;/h4&gt;
&lt;p&gt;&lt;code&gt;static int handle_upcalls(struct ofproto_dpif *ofproto, unsigned int max_batch)&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;功能&lt;/strong&gt;：接收upcall消息并对miss_upcall类型消息调用处理。&lt;/p&gt;
&lt;p&gt;函数中处理流程如下&lt;/p&gt;
&lt;p&gt;1 调用upcall消息接收函数：
    error = dpif_recv(ofproto-&amp;gt;dpif, upcall, buf);
 调用ofproto实体对应的成员组件实体dpif进行接收，把消息存入缓存buf中，并且把具体消息放入upcall中。具体分析见后。&lt;/p&gt;
&lt;p&gt;2 switch判断upcall消息的三种类型，并分别处理。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;对于SFLOW_UPCALL消息，调用handle_sflow_upcall处理，并ofpbuf_uninit(buf)清除缓存。&lt;/li&gt;
&lt;li&gt;对于BAD_UPCALL消息，直接清除缓存。&lt;/li&gt;
&lt;li&gt;对于MISS_UPCALL消息（重点），计数后，跳出switch用handle_miss_upcalls进行处理。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;3 对MISS_UPCALL消息调用handle_miss_upcalls进行着重处理。并紧后进行buf缓存清除。&lt;/p&gt;
&lt;p&gt;&lt;/br&gt;&lt;/p&gt;
&lt;h4&gt;&lt;strong&gt;2）消息接收函数dpif_recv（lib\dpif.c）&lt;/strong&gt;&lt;/h4&gt;
&lt;p&gt;&lt;code&gt;int dpif_recv(struct dpif *dpif, struct dpif_upcall *upcall, struct ofpbuf *buf)&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;关系&lt;/strong&gt;：handle_upcalls中调用此函数。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;功能&lt;/strong&gt;：与内核层通信获交互upcall消息。&lt;/p&gt;
&lt;p&gt;1 调用recv(dpif, upcall, buf)函数进行upcall消息接收，放入upcall中，至此得到upcall消息。其中通过指针dpif具体调用了底层的接收函数dpif_linux_recv（位于Lib\dpif-linux中），此函数中调用socket函数从内核汇总接收netlink的msg，存到ofpbuf中，即error = nl_sock_recv(ch-
&gt;sock, buf, false)，随后并调用error = parse_odp_packet(buf, upcall, &amp;amp;dp_ifindex)解析出upcall消息。&lt;/p&gt;
&lt;p&gt;2 &lt;code&gt;odp_flow_key_format(upcall-&amp;gt;key, upcall-&amp;gt;key_len, &amp;amp;flow)&lt;/code&gt;利用这个函数得到的upcall消息解析得到flow消息进行日志输出。&lt;/p&gt;
&lt;p&gt;&lt;/br&gt;&lt;/p&gt;
&lt;h4&gt;&lt;strong&gt;3) miss_upcall消息处理函数&lt;/strong&gt;&lt;/h4&gt;
&lt;p&gt;&lt;code&gt;static void handle_miss_upcalls(struct ofproto_dpif *ofproto, struct dpif_upcall *upcalls,size_t n_upcalls)&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;关系&lt;/strong&gt;：handle_upcalls中被调用&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;功能&lt;/strong&gt;：对miss_upcall消息具体进程处理。&lt;/p&gt;
&lt;p&gt;1 先利用for循环对upcall消息进行处理，为了从每个upcall的packet中提取出flow，然后比较flow，把具有相同flow的每个upcall的packet放入结构体flow_miss中，定义这个结构体变量为miss，然后对他们方便统一处理。&lt;/p&gt;
&lt;p&gt;循环中：利用函数ofproto_dpif_extract_flow_key从upcall消息中提取出flow（lib\flow.h中定义了这个数据结构），并利用函数flow_extract对flow的优先级、mark、in_port进行字段填充（实际中是对数据包协议各层进行逐层解析）。此后flow_hash(&amp;amp;miss-&amp;gt;flow, 0)对miss结构中的flow进行哈希处理，利用existing_miss = flow_miss_find(&amp;amp;todo, &amp;amp;miss-&amp;gt;flow, hash)把flow放入todo list的列表结构中。&lt;/p&gt;
&lt;p&gt;2 调用handle_flow_miss函数进行flow处理。&lt;/p&gt;
&lt;p&gt;&lt;code&gt;handle_flow_miss(ofproto, miss, flow_miss_ops, &amp;amp;n_ops)&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;传入ofproto和miss，对flow进行处理，得到处理结果类型flow_miss_ops，n_ops是得到的类型个数。得到处理类型用于后面根据这个类型进行处理。具体见函数。&lt;/p&gt;
&lt;p&gt;数据结构：flow_miss_op：此类型数据结构具有数据成员dpif_op这个数据结构（lib\dpif.c），定义了三种flow处理类型（dpif_flow_put\delete\execute）。&lt;/p&gt;
&lt;p&gt;3 调用&lt;code&gt;dpif_operate(ofproto-&amp;gt;dpif, dpif_ops, n_ops)&lt;/code&gt;函数，根据dpif_op中的处理类型进行分别三种处理（dpif_flow_put\delete\execute）。&lt;/p&gt;
&lt;p&gt;以上完成了从用户层获取upcall消息，并对MISS_UPCALL类型消息进行处理存入miss（结构体miss_flow）。至此，接下来要具体对miss进行处理（构造facet和对miss执行action）。&lt;/p&gt;</summary><category term="OVS souce code"></category></entry><entry><title>OVS源码分析——OpenFlow消息处理</title><link href="http://ysywh.github.io/pages/2015/05/02/OVS%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90%E2%80%94%E2%80%94OpenFlow%E6%B6%88%E6%81%AF%E5%A4%84%E7%90%86.html" rel="alternate"></link><updated>2015-08-30T17:30:00+08:00</updated><author><name>ysy</name></author><id>tag:ysywh.github.io,2015-05-02:pages/2015/05/02/OVS源码分析——OpenFlow消息处理.html</id><summary type="html">&lt;p&gt;&lt;/br&gt;&lt;/p&gt;
&lt;p&gt;OVS作为交换机，可以通过安全通道与控制器进行消息交互。控制器向OVS下发Openflow消息，可以完成对数据层面策略下发，流控制等功能。OVS接收到控制器下发的Openflow消息，需要对其进行接收，解析与处理，如安装流表项等操作。&lt;/p&gt;
&lt;hr /&gt;
&lt;p&gt;下文按照OVS接收处理openflow消息的处理流程进行分析：&lt;/p&gt;
&lt;p&gt;handle_openflow(ofproto\ofproto.c)开始对openflow消息进行具体处理，因此这个函数开始进行分析。首先说明一个重要数据结构。&lt;/p&gt;
&lt;p&gt;数据结构：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;ofconn(ofproto\connmgr.c)：代表ovs和控制器之间的一次openflow连接。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;主要的成员&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;connmgr（连接管理器）&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;rconn（一次可靠地openflow连接）&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;1 static bool  handle_openflow(struct ofconn *ofconn, struct ofpbuf *ofp_msg)&lt;/h4&gt;
&lt;p&gt;接收控制器下发的openflow消息，并且调用函数handle_openflow__进行消息类别判断，做出对应的处理。若接收到错误的openflow消息则会返回error，向控制器发“error消息”进行回复。&lt;/p&gt;
&lt;h4&gt;2 satic enum ofperr handle_openflow__(struct ofconn *ofconn, const struct ofpbuf *msg)&lt;/h4&gt;
&lt;p&gt;数据结构：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;ofp_header：openflow消息packets头部。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;数据成员&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;version（openflow消息版本号）&lt;/li&gt;
&lt;li&gt;type（openflow消息类型）&lt;/li&gt;
&lt;li&gt;length(openflow消息长度)&lt;/li&gt;
&lt;li&gt;xid（每次openflow消息id号，回复使用相同xid）。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;先从接收到的消息msg中提取出openflow消息头部msg-&amp;gt;data，赋值给oh。oh利用函数ofptype_decode获得openflow消息类型type（枚举型ofptype中罗列了各种消息类型，lib\ofp-msgs.h）。通过判别type找到与消息类型相符的具体消息处理函数，从而进行消息处理。比如下面详细分析一下flowmod消息。&lt;/p&gt;
&lt;h4&gt;3 static enum ofperr handle_flow_mod(struct ofconn *ofconn, const struct ofp_header *oh)&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;数据结构&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;ofputil_flow_mod（lib\ofp-util.h）：flowmod消息的数据结构&lt;ul&gt;
&lt;li&gt;数据成员&lt;ul&gt;
&lt;li&gt;match: 匹配域定义和掩码定义（match有两个成员flow（定义了可以匹配的各个字段）和flow_wildcards（bit掩码）)&lt;/li&gt;
&lt;li&gt;priority，cookie，command，buffer_id,out_port和&lt;/li&gt;
&lt;li&gt;ofpact：ofpact是action的头部，含有type等成员&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;通过函数ofpbuf_use_stub给ofpacts分配一块缓存opfbuf，然后通过调用函数ofputil_decode_flow_mod得到数据结构ofputil_flow_mod实体fm和实体ofpacts，则此时fm即包含flowmod消息格式的各个字段内容，而ofpacts缓存中含有action，并其中fm中的成员ofpacts指向这个缓存ofpacts。最后调用函数handle_flow_mod__，函数中，根据flowmod消息fm的command类型不同（五种），选择command相应流表项处理函数，在用户空间插入、修改或是删除流表项。&lt;/p&gt;
&lt;h4&gt;4 static enum ofperr add_flow(struct ofproto *ofproto, struct ofconn *ofconn,const struct ofputil_flow_mod *fm, const struct ofp_header *request)&lt;/h4&gt;
&lt;p&gt;选择flowmod指定id的表，根据flowmod消息创建分类流规则rule，并且插入rule，等再有相同流过来进行匹配，从而创建流表项。具体如下：&lt;/p&gt;
&lt;p&gt;先通过fm-&amp;gt;table_id获得flowmod指定id的流表，如果id号大于指定的最大流表数则会返回相应error类型。找到table id后，根据函数rule_alloc()分配一个分类器rule，随后调用函数cls_rule_init(&amp;amp;rule-&amp;gt;cr, &amp;amp;fm-&amp;gt;match, fm-&amp;gt;priority)填充rule-&amp;gt;cr，则rule的匹配域和优先级字段基本填充完成。之后进行冲突检测（check for overlap），防止重复添加相同rule。之后对rule各个字段进行填充，调用函数oftable_replace_rule插入这个初始化好的rule。&lt;/p&gt;
&lt;p&gt;这样addflow任务基本完成，等待以后相同流在核心层匹配失败后，upcall到用户层，则会根据匹配这个rule进行流表项的建立工作，如果不存在匹配的rule，则会丢弃或是上交controller，具体见用户态处理upcall消息分析。至于modify和delete类型flowmod消息，基本分析相同，如对command为modify的flowmod消息，则修改指定rule的action。&lt;/p&gt;</summary><category term="OVS souce code"></category></entry><entry><title>Cbench测试Opendaylight性能</title><link href="http://ysywh.github.io/pages/2015/04/03/Cbench%E6%B5%8B%E8%AF%95Opendaylight%E6%80%A7%E8%83%BD.html" rel="alternate"></link><updated>2015-08-30T17:30:00+08:00</updated><author><name>ysy</name></author><id>tag:ysywh.github.io,2015-04-03:pages/2015/04/03/Cbench测试Opendaylight性能.html</id><summary type="html">&lt;p&gt;&lt;/br&gt;&lt;/p&gt;
&lt;p&gt;要求和目标：用Cbench测试Opendaylight控制器的吞吐量和延时性能，拓扑简单设定为四个交换机&lt;/p&gt;
&lt;p&gt;实验通过Cbench模拟出拓扑，并对Opendaylight进行测试。&lt;/p&gt;
&lt;h3&gt;一：测试平台：&lt;/h3&gt;
&lt;p&gt;由于测试结果和测试平台的性能紧密联系，因此记录下测试平台参数。&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;硬件参数：&lt;/p&gt;
&lt;/blockquote&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;CPU: Intel(R) Xeon(R) CPU E5-2609 0 @ 2.40GHz&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Memory: 8GB DDR3 - 1600 Mhz&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;软件参数:&lt;/p&gt;
&lt;/blockquote&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;OS: Ubuntu 14.04&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Java: OpenJDK java version "1.7.0"&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Opendaylight版本:Helium-SR2&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Opendaylight设置为初始默认值&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;二：测试步骤&lt;/h3&gt;
&lt;p&gt;准备工作：先安装好&lt;a href="https://www.opendaylight.org/downloads"&gt;OpenDaylight&lt;/a&gt;和&lt;a href="https://github.com/deepurple/cbench-src"&gt;Cbench&lt;/a&gt;。&lt;/p&gt;
&lt;p&gt;1 开启opendaylight:&lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="http://7xnkv8.com1.z0.glb.clouddn.com/cbench1.png" /&gt;&lt;/p&gt;
&lt;p&gt;2 安装Opendaylight控制器测试组件：&lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="http://7xnkv8.com1.z0.glb.clouddn.com/cbench2.png" /&gt;&lt;/p&gt;
&lt;p&gt;说明：Opendaylight不同于其他控制器，需要测试组件的支持，且测试组件具有两种测试模式：RPC和data store模式（区别见最后说明），以下分别用Cbench进行吞吐量和延时的测试.&lt;/p&gt;
&lt;h3&gt;RPC模式性能测试&lt;/h3&gt;
&lt;p&gt;首先，在RPC模式下进行Opendaylight性能测试：&lt;/p&gt;
&lt;p&gt;1 在opendaylight中开启RPC模式的测试组件：&lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="http://7xnkv8.com1.z0.glb.clouddn.com/cbench3.png" /&gt;&lt;/p&gt;
&lt;p&gt;2 开启Cbench，测试控制器吞吐量(throughput)性能:&lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="http://7xnkv8.com1.z0.glb.clouddn.com/cbench4.png" /&gt;&lt;/p&gt;
&lt;p&gt;这里简单说明一下参数意义，s是交换机数量，M是每个交换机连接的主机数量，m是每次测试周期，l是测试循环次数，-t是进行吞吐量throughput测试，没有t则是延时latency测试。后面测试命令不再赘述。&lt;/p&gt;
&lt;p&gt;吞吐量测试结果截图：&lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="http://7xnkv8.com1.z0.glb.clouddn.com/cbench5.png" /&gt;&lt;/p&gt;
&lt;p&gt;测试结果简单解释：截图第二行显示测试性能的方面，可以看到是throughout，则是吞吐量测试。测试结果有10行数据，每行代表一次测试，每行有4个结果，单位是秒，代表每个交换机的测试结果，最后会有每毫秒的结果。最后一行是测试总的结果。&lt;/p&gt;
&lt;p&gt;3 开启Cbench，测试控制器延时（latency）性能：&lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="http://7xnkv8.com1.z0.glb.clouddn.com/cbench6.png" /&gt;&lt;/p&gt;
&lt;p&gt;测试结果截图：&lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="http://7xnkv8.com1.z0.glb.clouddn.com/cbench7.png" /&gt;&lt;/p&gt;
&lt;p&gt;特别说明：延迟测试结果要对测试的结果取倒数才是延迟。&lt;/p&gt;
&lt;h3&gt;data store模式性能测试&lt;/h3&gt;
&lt;p&gt;其次，在data store模式下进行Opendaylight性能测试:&lt;/p&gt;
&lt;p&gt;1关闭RPC模式，开启data store模式的测试组件：&lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="http://7xnkv8.com1.z0.glb.clouddn.com/cbench8.png" /&gt;&lt;/p&gt;
&lt;p&gt;2 开启Cbench，测试控制器吞吐量(throughput)性能:&lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="http://7xnkv8.com1.z0.glb.clouddn.com/cbench9.png" /&gt;&lt;/p&gt;
&lt;p&gt;吞吐量测试结果截图：&lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="http://7xnkv8.com1.z0.glb.clouddn.com/cbench10.png" /&gt;&lt;/p&gt;
&lt;p&gt;3 开启Cbench，测试控制器延时（latency）性能：&lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="http://7xnkv8.com1.z0.glb.clouddn.com/cbench11.png" /&gt;&lt;/p&gt;
&lt;p&gt;延迟测试结果截图：&lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="http://7xnkv8.com1.z0.glb.clouddn.com/cbench12.png" /&gt; &lt;/p&gt;
&lt;h3&gt;说明&lt;/h3&gt;
&lt;p&gt;1 两种模式特别说明，来自Opendaylight官网：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;RPC : programming flows directly through an RPC to the OF Plugin&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;data store : programming flows by writing them into the MD-SAL config space, from where they are picked up by the FRM and programmed into the plugin)&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;2 参数命令和结果截图说明见4和5，之后的不再赘述。&lt;/p&gt;
&lt;p&gt;3 测试结果的文本数据，见附件result.txt&lt;/p&gt;</summary><category term="Cbench OpenDaylight"></category></entry></feed>